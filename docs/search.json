[
  {
    "objectID": "online-bayesian-updating.html",
    "href": "online-bayesian-updating.html",
    "title": "David Grayson",
    "section": "",
    "text": "Problem\nRecently I had to solve a problem that involved online learning, i.e. updating the weights of your model only with incoming batches of data. This is opposed to the much more familiar offline training paradigm where you cycle through multiple epochs over a single dataset.\nThere’s lots of potential use cases for this - think of any situation where you want your model continuously updating as data is simultaneously coming in (e.g. streaming apps, stock market prediction, etc.). The problem arises that the ‘right’ way of updating your model depends heavily on how much you want to balance new vs old data in terms of its influence on the the model. On top of this, even if you want to treat all data points equally, a new learning rate must still be chosen at each update, and this is non-trivial given that you don’t know anything about future samples.\n\n\nProposed solution\nThis post will cover arguably the simplest and most robust approach of using a bayesian framework to update the posterior distribution with each new batch of data. The key insight here is that bayesian updates are implemented quite naturally by simply resetting the prior distribution to whatever the posterior was, and then recalculating the new posterior based on the new data. The updated “weights” (the way we think of them in a frequentist sense) correspond to the mode of the most recently updated posterior. Note that this method does treat all data points equally, regardless of time, but that’s a simplifying assumption that’s helpful for now.\nHere I’ll give an example using a simple logistic regression.\nWith a model like this, the updates are easy (and fast!) to calculate via quadratic approximation of the posterior in PyMC. Later I’ll mention some more generalizable frameworks for approximating the posterior with neural networks.\n\n\nImport\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport pymc as pm\nimport seaborn as sns\nfrom scipy.stats import norm\n\nWARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\n\n\nSimulate some data for a logistic regression\n(doesn’t matter how, as long as we can visualize a decision boundary)\nI’ll use 100 samples for simplicity\n\nn_obs = 100\n\nrng = np.random.default_rng()\nx0_obs = rng.normal(5, 2, size=n_obs)\ny_obs = rng.binomial(1, 0.5, size=n_obs)\n\ny_vec = np.zeros((len(y_obs), 2))\ny_vec[np.arange(len(y_obs)), y_obs] = 1\nmu_obs = y_vec @ np.array([1, 3]) + (y_vec @ np.array([1, 1])) * x0_obs\nx1_obs = rng.normal(mu_obs, 1)\n\nx0_obs = x0_obs - x0_obs.mean()\nx1_obs = x1_obs - x1_obs.mean()\nx_obs = np.concatenate(([np.ones(x0_obs.shape[0])], [x0_obs], [x1_obs])).T\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs)\nplt.show()\n\n\n\n\n\n\nSpecify the model & obtain the first posterior\nUsing just the first batch of 10 samples\n(and I’ll say to update the model a total of 10 times after that, 10 samples at a time)\n\nsteps = 10\n\nnum_features = x_obs.shape[1]\nstep = n_obs // steps\nx_start = x_obs[:step, :]\ny_start = y_obs[:step]\n\nwith pm.Model() as start_model:\n    # use Normal as priors\n    # w is our list of model weights\n    w = pm.Normal(\"w\", 0, 10, shape=num_features)\n    p = pm.math.invlogit(x_start @ w)\n    # likelihood function\n    y = pm.Binomial(\"y\", 1, p, observed=y_start)\n\n    # estimate the posterior as a gaussian\n    mean_w = pm.find_MAP()\n    hess = pm.find_hessian(mean_w, vars=[w])\n    var_cov = np.linalg.inv(hess)\n    std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:00<00:00 logp = -10.35, ||grad|| = 0.0050886]\n    \n    \n\n\n\n\n\n\n\nCode explanation\nLet me summarize what’s going on here:\nOur model has 3 weights (2 + bias), are denoted by w, and each given a prior with a mean of 0 and stdev of 10\nThe likelihood function is specified here as a bernoulli (or binomial with n=1) with p being a linear function of inputs * weights\nPyMC enables bayesian inference either through estimation or sampling of the posterior. In this case, we estimate it directly by finding the maximum of the posterior (analogous to maximum likelihood estimation) and describing its curvature with the hessian (a matrix of second-order partial derivatives). If we assume the shape of the posterior is gaussian, the hessian is sufficient to derive its standard deviation. Note that we could also sample the posterior instead, but that would be more expensive and won’t scale to n-dimensional problems.\nNotice then that the whole posterior for w is described by only two terms: mean_w and std_w (the mean/mode of a gaussian and its standard deviation). We can view their current values:\n\nprint(\"means:\", mean_w[\"w\"])\nprint(\"standard devs:\", std_w)\n\nmeans: [ 1.31026875 -5.99367736  6.3300888 ]\nstandard devs: [2.07364747 3.53659583 3.7744848 ]\n\n\n\n\nSubsequent updates\nFor subsequent updates, all we need is to reset our prior on w to the posterior we just found. The next posterior is discovered by simply repeating the same inference process on a new batch of data.\n\nmus, sigmas = [], []\n\nfor t in range(step, n_obs, step):\n    x_new = x_obs[t:t+step]\n    y_new = y_obs[t:t+step]\n    \n    with pm.Model() as updated_model:\n        # Reset priors to posteriors from previous iteration, unless weights are fixed\n        updated_mus = mean_w[\"w\"]\n        updated_sigmas = std_w\n        mus.append(updated_mus)\n        sigmas.append(updated_sigmas)\n        \n        w = pm.Normal(\"w\", updated_mus, updated_sigmas, shape=num_features)\n        p = pm.math.invlogit(x_new @ w)\n        y = pm.Binomial(\"y\", 1, p, observed=y_new)\n\n        mean_w = pm.find_MAP()\n        hess = pm.find_hessian(mean_w, vars=[w])\n        var_cov = np.linalg.inv(hess)\n        std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -6.6914, ||grad|| = 0.0018485]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -11.45, ||grad|| = 0.00042804]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -8.2299, ||grad|| = 2.5082]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [9/9 00:00<00:00 logp = -14.18, ||grad|| = 5.6728]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -3.598, ||grad|| = 1.3588]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.296, ||grad|| = 3.5075]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5569, ||grad|| = 2.7495]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5458, ||grad|| = 1.4423]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -6.5989, ||grad|| = 7.722]\n    \n    \n\n\n\n\n\nPlot changes to the posterior with each update\n\nmus = np.array(mus)\nsigmas = np.array(sigmas)\ncmap = matplotlib.cm.autumn\nx_axis = np.arange(-10, 10, 0.1)\n\nfor j in range(mus.shape[1]):\n    plt.figure(figsize=(4, 1))\n    plt.title(f\"w{j}\")\n    for c, (mu, sigma) in enumerate(zip(mus[:, j], sigmas[:, j])):\n        plt.plot(x_axis, norm.pdf(x_axis, mu, sigma), color=cmap(1 - c / mus.shape[0]))\n\n\n\n\n\n\n\n\n\n\nIn order to see the decision boundary, I need the weights as point estimates, so I just take the mean of the posterior.\nBelow shows how the decision boundary changed with each update.\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs.flatten())\n\nalpha0 = alpha = 1 / mus.shape[0]\nfor mu in mus:\n    w0, w1, w2 = mu\n    b = -w0/w2\n    m = -w1/w2\n    xd = np.array([x0_obs.min(), x0_obs.max()])\n    yd = m*xd + b\n    plt.plot(xd, yd, 'k', lw=1, ls='--', alpha=min(alpha, 1))\n    alpha += alpha0\n\nplt.show()\n\n\n\n\nThe figure shows the decision boundary change at each update, where the darker lines show the later updates. You can see the initial lines are a bit wonky, but as new data is added the updates converge on a good estimate of the boundary.\nAnd that’s it!\nFor further reading:\nJust as I’ve implemented this solution as a bayesian regression, the same approach could be taken using more complex (e.g. deep) models by implementing them as bayesian neural networks (e.g. check out pyro or this more lightweight and very impressive package). Because the parameters of a bayesian NN are described as posterior distributions, they could also be updated in an online fashion by continually resetting posteriors <-> priors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Musings",
    "section": "",
    "text": "Culture and skepticism\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen train, test, and deploy (really) diverge\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoctors are weak learners\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow to make a personal blog using jupyter notebooks\n\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\n\n\n\n\n  \n\n\n\n\nOnline learning through bayesian updating\n\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\nEy yo\n\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m David. I’m a machine learning engineer and applied ML scientist. My professional interests include computer vision, generative AI, and deep learning in biotech. I am a big believer in mentorship and servant leadership. I also am generally interested in topics at the intersection of science, AI, data, and epistemology. You can read some of my first musings in my blog that I’ve just started here (more to come!). Some hobbies I love include traveling to far-flung places with my bike and trying to set high scores on pinball machines :)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of California, Davis | Davis, CA | PhD in Neuroscience | Sept 2012 - May 2017\nCornell University | Ithaca, NY | B.A. in Computational Neuroscience | Aug 2004 - May 2008"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nBuzz Solutions | Senior ML Engineer | Jan 2023 - present\nLogic 20/20 | Lead Data Scientist | Jan 2021 - August 2022\nSystem1 Biosciences | Senior ML Scientist | Feb 2019 - April 2020\nIntuit | Senior Data Scientist - AI/ML | Sep 2017 - Jan 2019\n–"
  },
  {
    "objectID": "posts/online-bayesian-updating.html",
    "href": "posts/online-bayesian-updating.html",
    "title": "Bayesian updating with logistic regression",
    "section": "",
    "text": "Problem\nRecently I had to solve a problem that involved online learning, i.e. updating the weights of your model only with incoming batches of data. This is opposed to the much more familiar offline training paradigm where you cycle through multiple epochs over a single dataset.\nThere’s lots of potential use cases for this - think of any situation where you want your model continuously updating as data is simultaneously coming in (e.g. streaming apps, stock market prediction, etc.). The problem arises that the ‘right’ way of updating your model depends heavily on how much you want to balance new vs old data in terms of its influence on the the model. On top of this, even if you want to treat all data points equally, a new learning rate must still be chosen at each update, and this is non-trivial given that you don’t know anything about future samples.\n\n\nProposed solution\nThis post will cover arguably the simplest and most robust approach of using a bayesian framework to update the posterior distribution with each new batch of data. The key insight here is that bayesian updates are implemented quite naturally by simply resetting the prior distribution to whatever the posterior was, and then recalculating the new posterior based on the new data. The updated “weights” (the way we think of them in a frequentist sense) correspond to the mode of the most recently updated posterior. Note that this method does treat all data points equally, regardless of time, but that’s a simplifying assumption that’s helpful for now.\nHere I’ll give an example using a simple logistic regression.\nWith a model like this, the updates are easy (and fast!) to calculate via quadratic approximation of the posterior in PyMC. Later I’ll mention some more generalizable frameworks for approximating the posterior with neural networks.\n\n\nImport\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport pymc as pm\nimport seaborn as sns\nfrom scipy.stats import norm\n\n\n\nSimulate some data for a logistic regression\n(doesn’t matter how, as long as we can visualize a decision boundary)\nI’ll use 100 samples for simplicity\n\nn_obs = 100\n\nrng = np.random.default_rng()\nx0_obs = rng.normal(5, 2, size=n_obs)\ny_obs = rng.binomial(1, 0.5, size=n_obs)\n\ny_vec = np.zeros((len(y_obs), 2))\ny_vec[np.arange(len(y_obs)), y_obs] = 1\nmu_obs = y_vec @ np.array([1, 3]) + (y_vec @ np.array([1, 1])) * x0_obs\nx1_obs = rng.normal(mu_obs, 1)\n\nx0_obs = x0_obs - x0_obs.mean()\nx1_obs = x1_obs - x1_obs.mean()\nx_obs = np.concatenate(([np.ones(x0_obs.shape[0])], [x0_obs], [x1_obs])).T\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs)\nplt.show()\n\n\n\n\n\n\nSpecify the model & obtain the first posterior\nUsing just the first batch of 10 samples\n(and I’ll say to update the model a total of 10 times after that, 10 samples at a time)\n\nsteps = 10\n\nnum_features = x_obs.shape[1]\nstep = n_obs // steps\nx_start = x_obs[:step, :]\ny_start = y_obs[:step]\n\nwith pm.Model() as start_model:\n    # use Normal as priors\n    # w is our list of model weights\n    w = pm.Normal(\"w\", 0, 10, shape=num_features)\n    p = pm.math.invlogit(x_start @ w)\n    # likelihood function\n    y = pm.Binomial(\"y\", 1, p, observed=y_start)\n\n    # estimate the posterior as a gaussian\n    mean_w = pm.find_MAP()\n    hess = pm.find_hessian(mean_w, vars=[w])\n    var_cov = np.linalg.inv(hess)\n    std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:00<00:00 logp = -10.35, ||grad|| = 0.0050886]\n    \n    \n\n\n\n\n\n\n\nCode explanation\nLet me summarize what’s going on here:\nOur model has 3 weights (2 + bias), are denoted by w, and each given a prior with a mean of 0 and stdev of 10\nThe likelihood function is specified here as a bernoulli (or binomial with n=1) with p being a linear function of inputs * weights\nPyMC enables bayesian inference either through estimation or sampling of the posterior. In this case, we estimate it directly by finding the maximum of the posterior (analogous to maximum likelihood estimation) and describing its curvature with the hessian (a matrix of second-order partial derivatives). If we assume the shape of the posterior is gaussian, the hessian is sufficient to derive its standard deviation. Note that we could also sample the posterior instead, but that would be more expensive and won’t scale to n-dimensional problems.\nNotice then that the whole posterior for w is described by only two terms: mean_w and std_w (the mean/mode of a gaussian and its standard deviation). We can view their current values:\n\nprint(\"means:\", mean_w[\"w\"])\nprint(\"standard devs:\", std_w)\n\nmeans: [ 1.31026875 -5.99367736  6.3300888 ]\nstandard devs: [2.07364747 3.53659583 3.7744848 ]\n\n\n\n\nSubsequent updates\nFor subsequent updates, all we need is to reset our prior on w to the posterior we just found. The next posterior is discovered by simply repeating the same inference process on a new batch of data.\n\nmus, sigmas = [], []\n\nfor t in range(step, n_obs, step):\n    x_new = x_obs[t:t+step]\n    y_new = y_obs[t:t+step]\n    \n    with pm.Model() as updated_model:\n        # Reset priors to posteriors from previous iteration, unless weights are fixed\n        updated_mus = mean_w[\"w\"]\n        updated_sigmas = std_w\n        mus.append(updated_mus)\n        sigmas.append(updated_sigmas)\n        \n        w = pm.Normal(\"w\", updated_mus, updated_sigmas, shape=num_features)\n        p = pm.math.invlogit(x_new @ w)\n        y = pm.Binomial(\"y\", 1, p, observed=y_new)\n\n        mean_w = pm.find_MAP()\n        hess = pm.find_hessian(mean_w, vars=[w])\n        var_cov = np.linalg.inv(hess)\n        std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -6.6914, ||grad|| = 0.0018485]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -11.45, ||grad|| = 0.00042804]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -8.2299, ||grad|| = 2.5082]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [9/9 00:00<00:00 logp = -14.18, ||grad|| = 5.6728]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -3.598, ||grad|| = 1.3588]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.296, ||grad|| = 3.5075]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5569, ||grad|| = 2.7495]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5458, ||grad|| = 1.4423]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -6.5989, ||grad|| = 7.722]\n    \n    \n\n\n\n\n\nPlot changes to the posterior with each update\n\nmus = np.array(mus)\nsigmas = np.array(sigmas)\ncmap = matplotlib.cm.autumn\nx_axis = np.arange(-10, 10, 0.1)\n\nfor j in range(mus.shape[1]):\n    plt.figure(figsize=(4, 1))\n    plt.title(f\"w{j}\")\n    for c, (mu, sigma) in enumerate(zip(mus[:, j], sigmas[:, j])):\n        plt.plot(x_axis, norm.pdf(x_axis, mu, sigma), color=cmap(1 - c / mus.shape[0]))\n\n\n\n\n\n\n\n\n\n\nIn order to see the decision boundary, I need the weights as point estimates, so I just take the mean of the posterior.\nBelow shows how the decision boundary changed with each update.\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs.flatten())\n\nalpha0 = alpha = 1 / mus.shape[0]\nfor mu in mus:\n    w0, w1, w2 = mu\n    b = -w0/w2\n    m = -w1/w2\n    xd = np.array([x0_obs.min(), x0_obs.max()])\n    yd = m*xd + b\n    plt.plot(xd, yd, 'k', lw=1, ls='--', alpha=min(alpha, 1))\n    alpha += alpha0\n\nplt.show()\n\n\n\n\nThe figure shows the decision boundary change at each update, where the darker lines show the later updates. You can see the initial lines are a bit wonky, but as new data is added the updates converge on a good estimate of the boundary.\nAnd that’s it!\n\n\nFurther reading\nJust as I’ve implemented this solution as a bayesian regression, the same approach could be taken using more complex (e.g. deep) models by implementing them as bayesian neural networks (e.g. check out pyro or this more lightweight and very impressive package). Because the parameters of a bayesian NN are described as posterior distributions, they could also be updated in an online fashion by continually resetting posteriors <-> priors."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Ey yo",
    "section": "",
    "text": "I keep hearing from people about how important it is to blog. And I keep having ideas that I feel like I should put down on paper. So, here goes. Entry one. Done!"
  },
  {
    "objectID": "posts/online-bayesian-updating/online-bayesian-updating.html",
    "href": "posts/online-bayesian-updating/online-bayesian-updating.html",
    "title": "Online learning through bayesian updating",
    "section": "",
    "text": "Problem\nRecently I had to solve a machine learning problem that involved online learning, i.e. updating the weights of your model only with incoming batches of data. This is opposed to the much more familiar offline training paradigm where you cycle through multiple epochs over a single dataset.\nThere’s lots of potential use cases for this - think of any situation where you want your model continuously updating as data is simultaneously coming in (e.g. streaming apps, stock market prediction, etc.). The problem arises that the ‘right’ way of updating your model depends heavily on how much you want to balance new vs old data in terms of its influence on the the model. On top of this, even if you want to treat all data points equally, a new learning rate must still be chosen at each update, and this is non-trivial given that you don’t know anything about future samples.\nDo a quick search online and you will find lots of lengthy discussions of this problem as well as research papers that address it. Surprisingly, I found very few examples of simple proof-of-concepts with working code. Here I will aim to create just that.\n\n\nProposed solution\nThis post will show how to use bayesian inference to iteratively update a model’s weights with each new batch of data. The key insight here is that bayesian updates are implemented quite naturally by simply resetting the prior distribution to whatever the posterior was, and then recalculating the new posterior based on the new data. The updated “weights” (the way we think of them in a frequentist sense) correspond to the mode of the most recently updated posterior. Note that this method does treat all data points equally, regardless of time, but that’s a simplifying assumption that’s helpful for now.\nHere I’ll give an example using a simple logistic regression.\nWith a model like this, the updates are easy (and fast!) to calculate via quadratic approximation of the posterior in PyMC. Later I’ll mention some more generalizable frameworks for approximating the posterior with neural networks.\n\n\nImport\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport pymc as pm\nimport seaborn as sns\nfrom scipy.stats import norm\n\n\n\nSimulate some data for a logistic regression\n(doesn’t matter how, as long as we can visualize a decision boundary)\nI’ll use 100 samples for simplicity\n\nn_obs = 100\n\nrng = np.random.default_rng()\nx0_obs = rng.normal(5, 2, size=n_obs)\ny_obs = rng.binomial(1, 0.5, size=n_obs)\n\ny_vec = np.zeros((len(y_obs), 2))\ny_vec[np.arange(len(y_obs)), y_obs] = 1\nmu_obs = y_vec @ np.array([1, 3]) + (y_vec @ np.array([1, 1])) * x0_obs\nx1_obs = rng.normal(mu_obs, 1)\n\nx0_obs = x0_obs - x0_obs.mean()\nx1_obs = x1_obs - x1_obs.mean()\nx_obs = np.concatenate(([np.ones(x0_obs.shape[0])], [x0_obs], [x1_obs])).T\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs)\nplt.show()\n\n\n\n\n\n\nSpecify the model & obtain the first posterior\nUsing just the first batch of 10 samples\n(and I’ll say to update the model a total of 10 times after that, 10 samples at a time)\n\nsteps = 10\n\nnum_features = x_obs.shape[1]\nstep = n_obs // steps\nx_start = x_obs[:step, :]\ny_start = y_obs[:step]\n\nwith pm.Model() as start_model:\n    # use Normal as priors\n    # w is our list of model weights\n    w = pm.Normal(\"w\", 0, 10, shape=num_features)\n    p = pm.math.invlogit(x_start @ w)\n    # likelihood function\n    y = pm.Binomial(\"y\", 1, p, observed=y_start)\n\n    # estimate the posterior as a gaussian\n    mean_w = pm.find_MAP()\n    hess = pm.find_hessian(mean_w, vars=[w])\n    var_cov = np.linalg.inv(hess)\n    std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:00<00:00 logp = -10.35, ||grad|| = 0.0050886]\n    \n    \n\n\n\n\n\n\n\nCode explanation\nLet me summarize what’s going on here:\nOur model has 3 weights (2 + bias), are denoted by w, and each given a prior with a mean of 0 and stdev of 10\nThe likelihood function is specified here as a bernoulli (or binomial with n=1) with p being a sigmoid function of inputs * weights\nPyMC enables bayesian inference either through estimation or sampling of the posterior. In this case, we estimate it directly by finding the maximum of the posterior (analogous to maximum likelihood estimation) and describing its curvature with the hessian (a matrix of second-order partial derivatives). If we assume the shape of the posterior is gaussian, the hessian is sufficient to derive its standard deviation. Note that we could also sample the posterior instead, but that would be more expensive and won’t scale to n-dimensional problems.\nNotice then that the whole posterior for w is described by only two terms: mean_w and std_w (the mean/mode of a gaussian and its standard deviation). We can view their current values:\n\nprint(\"means:\", mean_w[\"w\"])\nprint(\"standard devs:\", std_w)\n\nmeans: [ 1.31026875 -5.99367736  6.3300888 ]\nstandard devs: [2.07364747 3.53659583 3.7744848 ]\n\n\n\n\nSubsequent updates\nFor subsequent updates, all we need is to reset our prior on w to the posterior we just found. The next posterior is discovered by simply repeating the same inference process on a new batch of data.\n\nmus, sigmas = [], []\n\nfor t in range(step, n_obs, step):\n    x_new = x_obs[t:t+step]\n    y_new = y_obs[t:t+step]\n    \n    with pm.Model() as updated_model:\n        # Reset priors to posteriors from previous iteration, unless weights are fixed\n        updated_mus = mean_w[\"w\"]\n        updated_sigmas = std_w\n        mus.append(updated_mus)\n        sigmas.append(updated_sigmas)\n        \n        w = pm.Normal(\"w\", updated_mus, updated_sigmas, shape=num_features)\n        p = pm.math.invlogit(x_new @ w)\n        y = pm.Binomial(\"y\", 1, p, observed=y_new)\n\n        mean_w = pm.find_MAP()\n        hess = pm.find_hessian(mean_w, vars=[w])\n        var_cov = np.linalg.inv(hess)\n        std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -6.6914, ||grad|| = 0.0018485]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -11.45, ||grad|| = 0.00042804]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -8.2299, ||grad|| = 2.5082]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [9/9 00:00<00:00 logp = -14.18, ||grad|| = 5.6728]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -3.598, ||grad|| = 1.3588]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.296, ||grad|| = 3.5075]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5569, ||grad|| = 2.7495]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5458, ||grad|| = 1.4423]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -6.5989, ||grad|| = 7.722]\n    \n    \n\n\n\n\n\nPlot changes to the posterior with each update\n\nmus = np.array(mus)\nsigmas = np.array(sigmas)\ncmap = matplotlib.cm.autumn\nx_axis = np.arange(-10, 10, 0.1)\n\nfor j in range(mus.shape[1]):\n    plt.figure(figsize=(4, 1))\n    plt.title(f\"w{j}\")\n    for c, (mu, sigma) in enumerate(zip(mus[:, j], sigmas[:, j])):\n        plt.plot(x_axis, norm.pdf(x_axis, mu, sigma), color=cmap(1 - c / mus.shape[0]))\n\n\n\n\n\n\n\n\n\n\nIn order to see the decision boundary, I need the weights as point estimates, so I just take the mean of the posterior.\nBelow shows how the decision boundary changed with each update.\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs.flatten())\n\nalpha0 = alpha = 1 / mus.shape[0]\nfor mu in mus:\n    w0, w1, w2 = mu\n    b = -w0/w2\n    m = -w1/w2\n    xd = np.array([x0_obs.min(), x0_obs.max()])\n    yd = m*xd + b\n    plt.plot(xd, yd, 'k', lw=1, ls='--', alpha=min(alpha, 1))\n    alpha += alpha0\n\nplt.show()\n\n\n\n\nThe figure shows the decision boundary change at each update, where the darker lines show the later updates. You can see the initial lines are a bit wonky, but as new data is added the updates converge on a good estimate of the boundary.\nAnd that’s it! Note that because it is actually the full posterior that is estimated, and not just the point estimate of its maximum, I can continue updating the model in perpetuity as new data arrives and still be able to recover the new decision boundary at any time.\n\n\nFurther reading\nJust as I’ve implemented this solution as a bayesian regression, the same approach could be taken using more complex (e.g. deep) models by implementing them as bayesian neural networks (e.g. check out pyro or this more lightweight and very impressive package). Because the parameters of a bayesian NN are described as posterior distributions, they could also be updated in an online fashion by continually resetting posteriors <-> priors."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "",
    "text": "So I made this personal website and blog after spending a fair amount of time researching how to publish a personal website and blog.\nSome options that were suggested to me:\n\nMedium\nWordpress\nGithub Pages + Jekyll\nGithub Pages + fastpages\n\nAfter realizing that I’d want tight integration between the tools I was using to produce content (i.e. jupyter notebooks) and the mechanism for publishing the content, I decided against Medium or Wordpress.\nGithub pages + jekyll was more what I wanted, but also turned up a few issues. 1. There was a fair amount to learn about jekyll to understand how to customize formatting. 2. It’s much easier to use an existing template, but the options out there generally are either not well-documented or not maintained, and 3. There’s a still an integration layer I’d need to manage in order to transfer code/outputs from notebooks to web content. I wanted SOME ability to customize, but mostly I wanted to get this thing up and running as fast as possible, and minimize potential work down the road.\nThose issues are what fastpages was designed to solve. It’s a well-documented, open-source package that enables automated rendering of content from a notebook into html content that is ready to publish, with some flexibility in how to render the content. It’s not a perfect solution - you’re still tied to their design templates. But there’s always a trade-off between customizability and ease of use, and this seemed like the trade-off that was right for me.\nBut, fastpages has been deprecated in favor of another open-source package."
  },
  {
    "objectID": "posts/01-27-2023-welcome/index.html",
    "href": "posts/01-27-2023-welcome/index.html",
    "title": "Ey yo",
    "section": "",
    "text": "I keep hearing from people about how important it is to blog. And I keep having ideas that I feel like I should put down on paper. So, here goes. Entry one. Done!"
  },
  {
    "objectID": "posts/01-28-2023-online-bayesian-updating/online-bayesian-updating.html",
    "href": "posts/01-28-2023-online-bayesian-updating/online-bayesian-updating.html",
    "title": "Online learning through bayesian updating",
    "section": "",
    "text": "Problem\nRecently I had to solve a machine learning problem that involved online learning, i.e. updating the weights of your model only with incoming batches of data. This is opposed to the much more familiar offline training paradigm where you cycle through multiple epochs over a single dataset.\nThere’s lots of potential use cases for this - think of any situation where you want your model continuously updating as data is simultaneously coming in (e.g. streaming apps, stock market prediction, etc.). The problem arises that the ‘right’ way of updating your model depends heavily on how much you want to balance new vs old data in terms of its influence on the the model. On top of this, even if you want to treat all data points equally, a new learning rate must still be chosen at each update, and this is non-trivial given that you don’t know anything about future samples.\nDo a quick search online and you will find lots of lengthy discussions of this problem as well as research papers that address it. Surprisingly, I found very few examples of simple proof-of-concepts with working code. Here I will aim to create just that.\n\n\nProposed solution\nThis post will show how to use bayesian inference to iteratively update a model’s weights with each new batch of data. The key insight here is that bayesian updates are implemented quite naturally by simply resetting the prior distribution to whatever the posterior was, and then recalculating the new posterior based on the new data. The updated “weights” (the way we think of them in a frequentist sense) correspond to the mode of the most recently updated posterior. Note that this method does treat all data points equally, regardless of time, but that’s a simplifying assumption that’s helpful for now.\nHere I’ll give an example using a simple logistic regression.\nWith a model like this, the updates are easy (and fast!) to calculate via quadratic approximation of the posterior in PyMC. Later I’ll mention some more generalizable frameworks for approximating the posterior with neural networks.\n\n\nImport\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport pymc as pm\nimport seaborn as sns\nfrom scipy.stats import norm\n\n\n\nSimulate some data for a logistic regression\n(doesn’t matter how, as long as we can visualize a decision boundary)\nI’ll use 100 samples for simplicity. The idea is that I’m going to use subsamples of this dataset to fit and update a model, one subsample at a time.\n\nn_obs = 100\n\nrng = np.random.default_rng()\nx0_obs = rng.normal(5, 2, size=n_obs)\ny_obs = rng.binomial(1, 0.5, size=n_obs)\n\ny_vec = np.zeros((len(y_obs), 2))\ny_vec[np.arange(len(y_obs)), y_obs] = 1\nmu_obs = y_vec @ np.array([1, 3]) + (y_vec @ np.array([1, 1])) * x0_obs\nx1_obs = rng.normal(mu_obs, 1)\n\nx0_obs = x0_obs - x0_obs.mean()\nx1_obs = x1_obs - x1_obs.mean()\nx_obs = np.concatenate(([np.ones(x0_obs.shape[0])], [x0_obs], [x1_obs])).T\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs)\nplt.show()\n\n\n\n\n\n\nSpecify the model & obtain the first posterior\nUsing just the first batch of 10 samples\n(and I’ll say to update the model a total of 10 times after that, 10 samples at a time)\n\nsteps = 10\n\nnum_features = x_obs.shape[1]\nstep = n_obs // steps\nx_start = x_obs[:step, :]\ny_start = y_obs[:step]\n\nwith pm.Model() as start_model:\n    # use Normal as priors\n    # w is our list of model weights\n    w = pm.Normal(\"w\", 0, 10, shape=num_features)\n    p = pm.math.invlogit(x_start @ w)\n    # likelihood function\n    y = pm.Binomial(\"y\", 1, p, observed=y_start)\n\n    # estimate the posterior as a gaussian\n    mean_w = pm.find_MAP()\n    hess = pm.find_hessian(mean_w, vars=[w])\n    var_cov = np.linalg.inv(hess)\n    std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:00<00:00 logp = -10.35, ||grad|| = 0.0050886]\n    \n    \n\n\n\n\n\n\n\nCode explanation\nLet me summarize what’s going on here:\nOur model has 3 weights (2 + bias), are denoted by w, and each given a prior with a mean of 0 and stdev of 10\nThe likelihood function is specified here as a bernoulli (or binomial with n=1) with p being a sigmoid function of inputs * weights\nPyMC enables bayesian inference either through estimation or sampling of the posterior. In this case, we estimate it directly by finding the maximum of the posterior (analogous to maximum likelihood estimation) and describing its curvature with the hessian (a matrix of second-order partial derivatives). If we assume the shape of the posterior is gaussian, the hessian is sufficient to derive its standard deviation. Note that we could also sample the posterior instead, but that would be more expensive and won’t scale to n-dimensional problems.\nNotice then that the whole posterior for w is described by only two terms: mean_w and std_w (the mean/mode of a gaussian and its standard deviation). We can view their current values:\n\nprint(\"means:\", mean_w[\"w\"])\nprint(\"standard devs:\", std_w)\n\nmeans: [ 1.31026875 -5.99367736  6.3300888 ]\nstandard devs: [2.07364747 3.53659583 3.7744848 ]\n\n\n\n\nSubsequent updates\nFor subsequent updates, all we need is to reset our prior on w to the posterior we just found. The next posterior is discovered by simply repeating the same inference process on a new batch of data.\n\nmus, sigmas = [], []\n\nfor t in range(step, n_obs, step):\n    x_new = x_obs[t:t+step]\n    y_new = y_obs[t:t+step]\n    \n    with pm.Model() as updated_model:\n        # Reset priors to posteriors from previous iteration, unless weights are fixed\n        updated_mus = mean_w[\"w\"]\n        updated_sigmas = std_w\n        mus.append(updated_mus)\n        sigmas.append(updated_sigmas)\n        \n        w = pm.Normal(\"w\", updated_mus, updated_sigmas, shape=num_features)\n        p = pm.math.invlogit(x_new @ w)\n        y = pm.Binomial(\"y\", 1, p, observed=y_new)\n\n        mean_w = pm.find_MAP()\n        hess = pm.find_hessian(mean_w, vars=[w])\n        var_cov = np.linalg.inv(hess)\n        std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -6.6914, ||grad|| = 0.0018485]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -11.45, ||grad|| = 0.00042804]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -8.2299, ||grad|| = 2.5082]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [9/9 00:00<00:00 logp = -14.18, ||grad|| = 5.6728]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -3.598, ||grad|| = 1.3588]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.296, ||grad|| = 3.5075]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5569, ||grad|| = 2.7495]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5458, ||grad|| = 1.4423]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -6.5989, ||grad|| = 7.722]\n    \n    \n\n\n\n\n\nPlot changes to the posterior with each update\n\nmus = np.array(mus)\nsigmas = np.array(sigmas)\ncmap = matplotlib.cm.autumn\nx_axis = np.arange(-10, 10, 0.1)\n\nfor j in range(mus.shape[1]):\n    plt.figure(figsize=(4, 1))\n    plt.title(f\"w{j}\")\n    for c, (mu, sigma) in enumerate(zip(mus[:, j], sigmas[:, j])):\n        plt.plot(x_axis, norm.pdf(x_axis, mu, sigma), color=cmap(1 - c / mus.shape[0]))\n\n\n\n\n\n\n\n\n\n\nIn order to see the decision boundary, I need the weights as point estimates, so I just take the mean of the posterior.\nBelow shows how the decision boundary changed with each update.\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs.flatten())\n\nalpha0 = alpha = 1 / mus.shape[0]\nfor mu in mus:\n    w0, w1, w2 = mu\n    b = -w0/w2\n    m = -w1/w2\n    xd = np.array([x0_obs.min(), x0_obs.max()])\n    yd = m*xd + b\n    plt.plot(xd, yd, 'k', lw=1, ls='--', alpha=min(alpha, 1))\n    alpha += alpha0\n\nplt.show()\n\n\n\n\nThe figure shows the decision boundary at each update, where the darker lines show the later updates. You can see the initial lines are a bit wonky, but as new data is added the updates converge on a good estimate of the boundary.\nAnd that’s it! Note that because it is actually the full posterior that is estimated, and not just the point estimate of its maximum, I can continue updating the model in perpetuity as new data arrives and still be able to recover point estimates of the weights at any time.\n\n\nFurther reading\nJust as I’ve implemented this solution as a bayesian regression, the same basic approach could be taken using more complex (e.g. deep) models by implementing them as bayesian neural networks (e.g. check out pyro or this more lightweight and very impressive package). Updating bayesian NNs will rely on more advanced methods than the quadratic approximations done here (e.g. variational inference), but these can also be implemented an online fashion by continually resetting posteriors <-> priors."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#solution",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#solution",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Solution",
    "text": "Solution\nEnter Quarto. This post will cover how to use Github Pages with Quarto in order to manage a personal website and blog with content that is generated entirely through jupyter notebooks. As you’ll see, what’s nice about this is that once you do the initial setup, all the content can be managed through nothing more than the notebooks themselves (if you’re into that sort of thing).\nSo let’s get to it."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-set-up-your-github-repo",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-set-up-your-github-repo",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 1: Set up your github repo",
    "text": "Step 1: Set up your github repo\nName it <username>.github.io and don’t include a README"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-create-your-github-repo",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-create-your-github-repo",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 1: Create your github repo",
    "text": "Step 1: Create your github repo\nName it <username>.github.io and don’t include a README. e.g.:\n[]create_gh_repo.png\nGo to the repo’s Pages settings and enable deployment of the main/master branch from the /docs folder\n[]edit_repo_settings.png"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-create-your-github-repo-1",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-create-your-github-repo-1",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 1: Create your github repo",
    "text": "Step 1: Create your github repo"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-2-initialize-your-quarto-site-locally",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-2-initialize-your-quarto-site-locally",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 2: Initialize your quarto site locally",
    "text": "Step 2: Initialize your quarto site locally\nclone your repo locally and create the initial quarto documents\n\ngit clone https://github.com/<username>/<username>.github.io\nquarto create-project <username>.github.io --type website:blog\n\nEdit the generated _quarto.yml file to use docs as the output-dir. It should read from the top:\nproject:\n  type: website\n  output-dir: docs\nThen, from the root of your repo, add a .nojekyll file to the root of your repository that tells GitHub Pages not to do additional processing of your published site using Jekyll\ntouch .nojekyll"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-3-add-some-content",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-3-add-some-content",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 3: add some content!",
    "text": "Step 3: add some content!\nFrom the root of your repo, begin rendering and previewing the pre-loaded content. As you add and edit your own content, your preview will update automatically.\n\n\nTerminal\n\nquarto preview"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-3-add-some-content-and-preview",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-3-add-some-content-and-preview",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 3: Add some content and preview",
    "text": "Step 3: Add some content and preview\nFrom the root of your repo, begin rendering and previewing the pre-loaded content in your browser. As you add and edit your own content, your preview will update automatically.\n\n\nTerminal\n\nquarto preview\n\nYou should see something that looks like this\n\nThe first thing to do is edit the title in the index.qmd file at the root level of your repo. You should see your update immediately.\n\n\nindex.qmd\n\n---\ntitle: \"<Your Name Here>\"\n\nNow you are ready to start adding posts. Unsurprisingly, all your posts should go in the posts folder.\nSo, in the posts folder, create a jupyter notebook, do stuff, and save. You should already see a new entry in your Home page on top of the other two pre-loaded posts.\nCool, right?\nN.B.: If you add a Raw cell to the top of your notebook, you can give it a title and date that will also appear on your site. Important: Make sure the cell is marked as “Raw” and not “Code” or “Markdown”. For example this is the header cell used in the notebook that this post comes from:\n\n\nposts/blog-post-about-posting-blogs.ipynb\n\n---\ntitle: \"How to make a personal blog using jupyter notebooks\"\ndate: \"2023-01-29\"\n---\n\nN.B.: To remove the pre-loaded posts, simply delete their folders in posts"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-push-to-github",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-push-to-github",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 4: Push to github",
    "text": "Step 4: Push to github\nYou can push all the content in your repo.\n\n\nTerminal\n\ngit add --all\ngit commit -m \"initial site\"\ngit push origin main"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-edit-your-homepage",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-edit-your-homepage",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 4: edit your homepage",
    "text": "Step 4: edit your homepage"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-5-push-to-github",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-5-push-to-github",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 5: Push to github",
    "text": "Step 5: Push to github\nFirst, add a .nojekyll file to the root of your repo that tells GitHub Pages not to do additional processing of your published site using Jekyll\n\n\nTerminal\n\ntouch .nojekyll\n\nThen, create a .gitignore file and add the output directory of your project to it:\n\n\n.gitignore\n\n/.quarto/\n/_site/\n\nNow you can push everything\n\n\nTerminal\n\ngit add --all\ngit commit -m \"initial site\"\ngit push origin main\n\n\nYour commit should automatically trigger a deployment through Github Actions. Once this completes, you should be able to see your site live at https://<username>.github.io\nAnd you’re done! You’ve now created a personal blog and can create additional posts by just creating new notebooks in the posts folder.\nTo learn more about how to customize your site appearance I highly recommend reading through quarto’s blog guide first. See also the full quarto guide."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-edit-your-about-page",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-edit-your-about-page",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 4: Edit your “About” page",
    "text": "Step 4: Edit your “About” page\nFrom the root of your repo open about.qmd and add some content about yourself. You can read more here about how to customize this page to your liking"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "",
    "text": "So I made this personal website and blog after spending a fair amount of time researching how to publish a personal website and blog.\nSome options that were suggested to me:\n\nMedium\nWordpress\nGithub Pages + Jekyll\nGithub Pages + fastpages\n\nAfter realizing that I’d want tight integration between the tools I was using to produce content (i.e. jupyter notebooks) and the mechanism for publishing the content, I decided against Medium or Wordpress.\nGithub pages + jekyll was more what I wanted, but also turned up a few issues. 1. There was a fair amount to learn about jekyll to understand how to customize formatting. 2. It’s much easier to use an existing template, but the options out there generally are either not well-documented or not maintained, and 3. There’s a still an integration layer I’d need to manage in order to transfer code/outputs from notebooks to web content. I wanted SOME ability to customize, but mostly I wanted to get this thing up and running as fast as possible, and minimize potential work down the road.\nThose issues are what fastpages was designed to solve. It’s a well-documented, open-source package that enables automated rendering of content from a notebook into html content that is ready to publish, with some flexibility in how to render the content. It’s not a perfect solution - you’re still tied to their design templates. But there’s always a trade-off between customizability and ease of use, and this seemed like the trade-off that was right for me.\nBut, fastpages has been deprecated in favor of another open-source package."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#solution",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#solution",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Solution",
    "text": "Solution\nEnter Quarto. This post will cover how to use Github Pages with Quarto in order to manage a personal website and blog with content that is generated entirely through jupyter notebooks. It took me some time to get through all the docs in github and quarto that explain how to do this, so for your convenience I’m condensing the setup process I went through into one place here. As you’ll see, what’s nice is that after the initial setup, all the content can be managed through nothing more than the notebooks themselves (if you’re into that sort of thing).\nIn fact, this very blog was made using the same steps, and you can view its source code here.\nSo let’s get to it."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-1-create-your-github-repo",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-1-create-your-github-repo",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 1: Create your github repo",
    "text": "Step 1: Create your github repo\nName it <username>.github.io. e.g.:\n\nThe URL for the site will be the same: https://username.github.io\nThen, go to the repo’s Pages settings and enable deployment of the main/master branch from the /docs folder"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-2-initialize-your-quarto-site-locally",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-2-initialize-your-quarto-site-locally",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 2: Initialize your quarto site locally",
    "text": "Step 2: Initialize your quarto site locally\nFirst, install quarto\nThen, clone your repo locally and create the initial quarto documents\n\n\nTerminal\n\ngit clone https://github.com/<username>/<username>.github.io\nquarto create-project <username>.github.io --type website:blog\ncd <username>.github.io\n\nReplacing <username> with your actual username of course\nFinally, edit the generated _quarto.yml file to use docs as the output-dir.\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-3-add-some-content-and-preview",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-3-add-some-content-and-preview",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 3: Add some content and preview",
    "text": "Step 3: Add some content and preview\nFirst, begin rendering and previewing the pre-loaded content in your browser. As you add and edit your own content, your preview will update automatically. From the root of your repo:\n\n\nTerminal\n\nquarto preview\n\nYou should see something that looks like this\n\n\n\nEdit the title in index.qmd. You should see your site update immediately.\n\n\nindex.qmd\n\n---\ntitle: \"<Your Name Here>\"\n\nNow you are ready to start adding posts. Unsurprisingly, all your posts should go in the posts folder.\nSo, in the posts folder, create a jupyter notebook, do stuff, and save. You should already see a new entry in your Home page on top of the other two pre-loaded posts. The notebook itself is used to render the post.\nCool, right?\nN.B.: If you add a Raw cell to the top of your notebook, you can give it a title and date that will also appear on your site. Important: Make sure the cell is marked as “Raw” and not “Code” or “Markdown”. For example this is the header cell used in the notebook that this post comes from:\n\n\nposts/blog-post-about-posting-blogs.ipynb\n\n---\ntitle: \"How to make a personal blog using jupyter notebooks\"\ndate: \"2023-01-29\"\n---\n\nN.B.: To remove the pre-loaded posts, simply delete their folders in posts"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-4-edit-your-about-page",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-4-edit-your-about-page",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 4: Edit your “About” page",
    "text": "Step 4: Edit your “About” page\nOpen about.qmd and add some content about yourself. You can read more here about how to customize this page to your liking"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-5-push-to-github",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-5-push-to-github",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 5: Push to github",
    "text": "Step 5: Push to github\nFirst, add a .nojekyll file to the root of your repo that tells GitHub Pages not to do additional processing of your published site using Jekyll\n\n\nTerminal\n\ntouch .nojekyll\n\nThen, create a .gitignore file and add the output directory of your project to it:\n\n\n.gitignore\n\n/.quarto/\n/_site/\n\nNow you can push everything\n\n\nTerminal\n\ngit add --all\ngit commit -m \"initial site\"\ngit push origin main\n\n\nYour commit should automatically trigger a deployment through Github Actions. Once this completes, you should be able to see your site live at https://<username>.github.io\nAnd you’re done! You’ve now created a personal blog and can create additional posts by just creating new notebooks under posts.\nTo learn more about how to customize your site appearance I highly recommend reading through quarto’s blog guide first. See also the full quarto guide."
  },
  {
    "objectID": "posts/02-02-2023-doctors-are-weak-learners/doctors-are-weak-learners.html",
    "href": "posts/02-02-2023-doctors-are-weak-learners/doctors-are-weak-learners.html",
    "title": "Doctors are weak learners",
    "section": "",
    "text": "Most of us have experienced a medical misdiagnosis of some kind. It is a frustrating experience that can result in enormous time wasted with unnecessary office visits and medical tests, plus significant stress and prolonged illness.\nAt the heart of the matter is the fact that many illnesses are still diagnosed using primarily human expertise with little understanding of the uncertainty behind those predictions. Even when lab tests are involved, interpretation of results and diagnostic conclusions still usually rely on the judgment of individual doctors. For instance, medical imaging such as MRIs or CT scans usually involves a trained radiologist reading the result and writing up a conclusion, which is then interpreted and communicated by the patient’s provider. The diagnosis is therefore the result of expert intuition from a select pair of trained specialists. Equivalently, it is the result of these experts’ combined knowledge and training, and also their lack of knowledge and lack of training.\nIt’s easy to see why this could be improved, and we have data that proves just how bad this process can be. Back in 2018, it was first shown that AI outperforms most expert dermatologists at detecting skin cancer, and reams of papers have come out since then improving on those results. If we go a bit further back to 2015, we find evidence that individual dermatologists misdiagnose at much higher rates than they would if they combined their votes on each diagnosis (though this ‘collective intelligence’ is still much worse than AI). The authors argue that this effect is due to each doctor having their own biases - their own way of looking at the same signs, symptoms, and test results, and drawing their own unique and often incorrect conclusions.\nThis shouldn’t really come as a surprise to anyone. Each doctor’s understanding is innately limited by human abilities of perception and cognitive integration between diverse forms of observation and patient data. In addition, each doctor can only practically be exposed to a (very) small subset of all the available patient data that might potentially be relevant to their training. In other words, each doctor can only fit their own limited mental model of disease to a limited sample of the data, leading to different doctors having different diagnostic models that are all suboptimal in their own unique way.\nIn the parlance of machine learning and artificial intelligence, these kinds of models are termed ‘weak learners’. Concretely, a weak learner is an oversimplified statistical model that has limited mathematical complexity and often only allowed to ‘see’ a limited subset of data.\nBasically, each doctor is training their own weak learner - a diagnostic model based on limited (i.e. weak) ability and experiential training.\nWhat’s fascinating is that, in the ML literature, weak learners actually serve a very important role in producing very high performing models. The idea is that you can use an ensemble approach, which means training many weak learners on the same task and then making final predictions by taking a vote among your learners. In fact, for many types of problems, particularly those where the data is tabular, these types of models are consistently the best performing. But the key takeaway is that you need to ensemble across many, many of these weak learners (i.e. hundreds or thousands depending on the size of the data). You also get another powerful boost to your model’s accuracy if you train the learners sequentially so that each subsequent learner is an improvement of the previous.\nOne of the findings of the 2015 skin cancer study was that, indeed, if you ensemble the predictions of all doctors, you get predictions that are on average far better than those of any individual doctor’s. Even so, AI now vastly outperforms the best human experts at skin cancer detection as well as a host of other imaging based diagnostics. Thus, both theory and data would suggest that ‘weak learner’ is an apt description of the innate limitations of individual doctors’ (read: any individual human’s) judgment and decision-making ability. The implication is that our error-prone healthcare system has been poorly serving patients by encouraging them to rely on their doctor’s opinion. That might be better than ceding authority to the ever expanding universe of medical quackery, but it’s much worse than providing data about how inaccurate doctors really are, and making an earnest effort to provide access to modern, science-based diagnostic approaches."
  },
  {
    "objectID": "posts/when-test-and-deploy-diverge.html",
    "href": "posts/when-test-and-deploy-diverge.html",
    "title": "When train, test, and deploy (really) diverge",
    "section": "",
    "text": "A common problem in machine learning is when your training data and real-world data diverge. One of the ways they can diverge is in their class balances.\nFor example, let’s say you are building a model to help a semiconductor factory to detect defects in their microchips before they are shipped off to their customers. Let’s say they are only expecting 1 such defect for every 10,000 chips they produce. Now, let’s also say you have procured a dataset of 300 microchips with defects. You would then need 300 * 10000 = 3,000,000 more images for your training dataset, each of them inspected to make sure they aren’t defective, to match the real class balance. Instead, you include just a few thousand such images, reasoning that this kind of model should easily converge with a more balanced dataset anyway.\nBut this poses a problem downstream. If the class balance of the training data and real world data diverge (in this case, 1:10 vs 1:10,000), the model’s behavior during deployment will be poorly calibrated. Here it expects the non-defect class to appear 1000 times more frequently than it actually does, which will cause an excessive false positive rate (low precision) at runtime. Simply put, you run the risk of saying there’s 1000 times more broken chips than there are.\nTherefore, we must take additional steps to estimate the model’s theoretical runtime (deployment) accuracy.\nOne approach is to run your evaluation metrics using a sample of test data with the correct class balance (i.e. 1:10,000). You can then choose your decision threshold based on the precision-recall or ROC curve generated on this test data, and feel good that you know approximately the runtime precision/recall/tpr/fpr when deploying your model with this same threshold.\nHowever, this might not be feasible in every scenario. Perhaps you don’t have access to all the millions of non-defect images taken or do not have resources to label that many images. Or even if you did, perhaps your class balance is so extreme that it’s simply not computationally tractable to reproduce. For example, let’s imagine you want to generate early earthquake warnings by building a model that uses 10 seconds of high-resolution seismic data. In California, there may only be 2-3 earthquakes of any significance per year, but there are 3,153,600 distinct 10s windows in a year, each of which could be considered a negative example. Multiply that by the dozens of different geolocations you would want different alert systems for and… you get the idea. This is such an extreme imbalance that you likely would never use all negative examples even for testing purposes.\nAs before, our training dataset has a different class balance relative to the deployment scenario, but now, so does our test data. We still need a way to estimate the model’s theoretical accuracy during runtime (deployment). How?\nWell, there’s good news and bad news, and they’re both the same. It depends on which accuracy metric. Since recall is already conditioned on the positive class, it is insensitive to class ratio, therefore the true recall at any given threshold is already estimated in the test data regardless of imbalance. Precision, because it is conditioned on the predicted class, is what’s influenced. Recall specifically that:\n\\(Recall = P(\\hat{y}=1|y=1)\\)\n\\(Precision = P(y=1|\\hat{y}=1)\\)\nIf these are the two metrics we care about, then we only need to adjust our estimate of the model’s precision during deployment. Let’s begin with\n\\(Precision_{test} = TP_{test} / (TP_{test} + FP_{test})\\)\nWhere \\(TP_{test}\\) and \\(FP_{test}\\) are the total number of true positives and false positives in the test dataset.\nLet’s assume now that we know the class ratios in both the test dataset as well as in the deployed scenario. Since it is the negative class that is upsampled during deployment, we simulate this by keeping the value for TP fixed and multiplying the value for FP (since these are negative class examples) by the fold change in negative over positive class ratio.\n\\(FP_{deploy} = FP_{test} * \\frac{neg_{deploy} / pos_{deploy}}{neg_{test} / pos_{test}}\\)\nWhere \\(neg\\) and \\(pos\\) refer to the counts of negative and positive examples within either the test data or deployment scenario.\nThen we get\n\\(Precision_{deploy} = TP_{test} / (TP_{test} + FP_{deploy})\\)\nAs you generate the precision-recall curve using your test data, you can use this as your corrected precision value. Again, recall is unchanged. The curve corresponds to the theoretical behavior of the model during deployment.\nLet’s take our microchip example again to see just how big of an impact the difference in class balance can have.\nLet’s say we want a precision of 0.33 when deployed, and the difference in class balance between train/test and deployment is still 1000-fold.\nSolving for \\(0.33 = TP / (TP + FP*1000)\\) gives us \\(FP = 1.33*TP / 330\\), which gives us an initial (uncorrected) precision of \\(1 / (1 + 1.33 / 330) = 0.996\\).\nThat means that at a (nearly perfect) precision of 0.996 in the test data, the same threshold during deployment has a (pretty bad) precision of 0.33.\nFinally, keep in mind that this method makes some pretty strong assumptions:\n\nYour training and test datasets are random samples of your deployment scenario\nYou know the “true” class balance in your deployment scenario\nThe model’s probabilities are well calibrated (they reflect their true likelihoods) (see here). That’s especially important for higher probabilities, where often models are not so well calibrated! In my experience, this is often where things get quite tricky. It is worth doing some research to see whether your model can be better calibrated with the highest confidence predictions.\n\nSome or all these assumptions are frequently wrong. Ultimately, there’s no equal substitute for having a correctly balanced representation of your real world data. However, that’s often not feasible, and in those cases this will help get a better approximation of your model’s real world behavior."
  },
  {
    "objectID": "posts/02-03-2023-when-test-and-deploy-diverge/when-test-and-deploy-diverge.html",
    "href": "posts/02-03-2023-when-test-and-deploy-diverge/when-test-and-deploy-diverge.html",
    "title": "When train, test, and deploy (really) diverge",
    "section": "",
    "text": "A common problem in machine learning is when your training data and real-world data diverge. One of the ways they can diverge is in their class balances.\nFor example, let’s say you are building a model to help a semiconductor factory to detect defects in their microchips before they are shipped off to their customers. Let’s say they are only expecting 1 such defect for every 10,000 chips they produce. Now, let’s also say you have procured a dataset of 300 microchips with defects. You would then need 300 * 10000 = 3,000,000 more images for your training dataset, each of them inspected to make sure they aren’t defective, to match the real class balance. Instead, you include just a few thousand such images, reasoning that this kind of model should easily converge with a more balanced dataset anyway.\nBut this poses a problem downstream. If the class balance of the training data and real world data diverge (in this case, 1:10 vs 1:10,000), the model’s behavior during deployment will be poorly calibrated. Here it expects the non-defect class to appear 1000 times more frequently than it actually does, which will cause an excessive false positive rate (low precision) at runtime. Simply put, you run the risk of saying there’s 1000 times more broken chips than there are.\nTherefore, we must take additional steps to estimate the model’s theoretical runtime (deployment) accuracy.\nOne approach is to run your evaluation metrics using a sample of test data with the correct class balance (i.e. 1:10,000). You can then choose your decision threshold based on the precision-recall or ROC curve generated on this test data, and feel good that you know approximately the runtime precision/recall/tpr/fpr when deploying your model with this same threshold.\nHowever, this might not be feasible in every scenario. Perhaps you don’t have access to all the millions of non-defect images taken or do not have resources to label that many images. Or even if you did, perhaps your class balance is so extreme that it’s simply not computationally tractable to reproduce. For example, let’s imagine you want to generate early earthquake warnings by building a model that uses 10 seconds of high-resolution seismic data. In California, there may only be 2-3 earthquakes of any significance per year, but there are 3,153,600 distinct 10s windows in a year, each of which could be considered a negative example. Multiply that by the dozens of different geolocations you would want different alert systems for and… you get the idea. This is such an extreme imbalance that you likely would never use all negative examples even for testing purposes.\nAs before, our training dataset has a different class balance relative to the deployment scenario, but now, so does our test data. We still need a way to estimate the model’s theoretical accuracy during runtime (deployment). How?\nWell, there’s good news and bad news, and they’re both the same. It depends on which accuracy metric. Since recall is already conditioned on the positive class, it is insensitive to class ratio, therefore the true recall at any given threshold is already estimated in the test data regardless of imbalance. Precision, because it is conditioned on the predicted class, is what’s influenced. Recall specifically that:\n\\(Recall = P(\\hat{y}=1|y=1)\\)\n\\(Precision = P(y=1|\\hat{y}=1)\\)\nIf these are the two metrics we care about, then we only need to adjust our estimate of the model’s precision during deployment. Let’s begin with\n\\(Precision_{test} = TP_{test} / (TP_{test} + FP_{test})\\)\nWhere \\(TP_{test}\\) and \\(FP_{test}\\) are the total number of true positives and false positives in the test dataset.\nLet’s assume now that we know the class ratios in both the test dataset as well as in the deployed scenario. Since it is the negative class that is upsampled during deployment, we simulate this by keeping the value for TP fixed and multiplying the value for FP (since these are negative class examples) by the fold change in negative over positive class ratio.\n\\(FP_{deploy} = FP_{test} * \\frac{neg_{deploy} / pos_{deploy}}{neg_{test} / pos_{test}}\\)\nWhere \\(neg\\) and \\(pos\\) refer to the counts of negative and positive examples within either the test data or deployment scenario.\nThen we get\n\\(Precision_{deploy} = TP_{test} / (TP_{test} + FP_{deploy})\\)\nAs you generate the precision-recall curve using your test data, you can use this as your corrected precision value. Again, recall is unchanged. The curve corresponds to the theoretical behavior of the model during deployment.\nLet’s take our microchip example again to see just how big of an impact the difference in class balance can have.\nLet’s say we want a precision of 0.33 when deployed, and the difference in class balance between train/test and deployment is still 1000-fold.\nSolving for \\(0.33 = TP / (TP + FP*1000)\\) gives us \\(FP = 1.33*TP / 330\\), which gives us an initial (uncorrected) precision of \\(1 / (1 + 1.33 / 330) = 0.996\\).\nThat means that at a (nearly perfect) precision of 0.996 in the test data, the same threshold during deployment has a (pretty bad) precision of 0.33.\nFinally, keep in mind that this method makes some pretty strong assumptions:\n\nYour training and test datasets are random samples of your deployment scenario\nYou know the “true” class balance in your deployment scenario\nThe model’s probabilities are well calibrated (they reflect their true likelihoods) (see here). That’s especially important for higher probabilities, where often models are not so well calibrated! In my experience, this is often where things get quite tricky. It is worth doing some research to see whether your model can be better calibrated with the highest confidence predictions.\n\nSome or all these assumptions are frequently wrong. Ultimately, there’s no equal substitute for having a correctly balanced representation of your real world data. However, that’s often not feasible, and in those cases this will help get a better approximation of your model’s real world behavior."
  },
  {
    "objectID": "posts/02-14-2023-culture-and-skepticism/culture-and-skepticism.html",
    "href": "posts/02-14-2023-culture-and-skepticism/culture-and-skepticism.html",
    "title": "Culture and skepticism",
    "section": "",
    "text": "A while back I read Sam’s Harris’ short book titled “Lying.” It deals with all the ways in which people, in everyday life, skirt the truth with each other. It argues, in Harris’ quintessentially provocative but rigorous style, that even the smallest lies we tell each other might not be so small. Harris points out that social norms deem acceptable a variety of forms of dishonesty, for example “white lies” and lies of omission. He argues that even these seemingly innocuous fibs are virtually always detrimental on the whole. I won’t go into all the reasoning behind that, or whether I agree, but it got me thinking more broadly about the tension between objective truth and social norms. It occurred to me that there are other types of social norms, apart from those dealing with honesty, that may also end up cheating us out of a clearer understanding of ourselves, and of the world around us.\nYears ago, I was at a gathering with friends and friends of friends. At some point, we went around and each person described some past story or future aspiration that mattered on a personal level. One person, let’s call him Robert, recounted a recent conversation with an acquaintance that he hoped to be better friends with, and who considered himself to be pretty knowledgeable about geology. According to Robert, this acquaintance asserted some supposed geological facts that seemed difficult to believe, and a combination of curiosity and skepticism drove Robert to ask deeper and deeper questions. Eventually, the acquaintance became frustrated and upset, feeling that his expertise was being questioned. They both left the conversation feeling dissatisfied, neither apologizing or attempting to reconcile in the moment. Later, Robert took the initiative to apologize.\nRobert framed this as an example of personal growth, in which he learned to prioritize a relationship he cared about by swallowing his pride. Everyone nodded, affirming him for taking the bold step of making himself vulnerable.\nSomething about this consensus seemed off to me. It seemed paradoxical to praise Robert for renouncing such an intellectually constructive habit. If his conversation partner actively prevented dialogue, shouldn’t that be seen as destructive? Openness and vulnerability are worthy habits, but shouldn’t the person lacking them be the one to apologize? I offered my challenge to the room, and was not surprised to find that it was poorly received. Most everyone agreed it was more or less natural and justified to feel offended when you are interrogated about something you believe you have an expertise in. Moreover, if you want to make friends and keep friends, you have to put your ego aside sometimes.\nFrom my experience, this kind of attitude is broadly popular. When relationships are at stake, you can’t be too much of a stickler. Being permissive is being practical, and considerate. And if someone is personally invested in some version of the truth, that’s a minefield you might not want to march through. Practical indeed, but also somewhat cynical.\nI see this broader attitude as a consequence of a pervasive kind of social taboo - that is, of challenging someone’s beliefs in a social setting. The corollary to this taboo is a status quo which deems that we ought to tolerate all beliefs that are central to anyone’s identity (assuming they’re not outrightly offensive). It’s a status quo that’s as contemptible as it is difficult to subvert. Who can deny the awkwardness they feel during Thanksgiving dinner when an aunt or uncle starts up about conspiracy theories or pseudoscience quackery? Who hasn’t sat silent or tried to abruptly change the subject at that moment, out of convenience? And what happens when it’s someone you are closer to, or someone you depend more upon - truth inevitably gets further downgraded.\nWe decline to challenge others’ beliefs because it’s unpleasant. But this is a path-of-least-resistance fallacy where we avoid social discomfort only to end up in an even worse place. For example, sitting silent or changing topics with our gullible relatives certainly would not have helped to curb the other pandemic that was happening simultaneously with COVID - the pandemic of conspiracy theory lunacy about COVID (i.e. that it wasn’t real, or wasn’t dangerous, or that the vaccines weren’t helpful, or that onions cure COVID). People died from COVID, yes, but millions also died from this lunacy.\nIntellectual honesty takes work. Knowledge takes work. Approving others’ questionable beliefs is helpful for building loyalty but not very useful for helping each other refine our understanding of the world. What if, instead of being preoccupied with loyalty, we encouraged each other to see skepticism as friendly and collaborative?\nThere are examples that show that this kind of shift in cultural expectations is possible. One that comes to mind is the concept of “cooperative overlapping”, a conversational style marked by frequent interrupting. Basically, each conversation partner attempts to grasp the other’s idea as quickly as possible and interrupts as a way to signal their understanding and build upon it (this is prominent for example in northeastern jewish culture). While much of western culture would take any form of interruption as a sign of disrespect, in the in-group it is quite the contrary. It is appreciated as a sign of mutual engagement.\nWhat if the same kind of reversal was possible with our expectations about whether people should believe the things we say? We reach real understanding by going through the process of questioning assumptions, validating those assumptions, questioning the credibility of sources, and validating those sources. In order to go through that process, we have to collectively lean towards being skeptical of others’ claims and of our own. By prioritizing loyalty and by obfuscating the limits of our knowledge, we deny each other this process. In doing so, we botch opportunities left and right to arrive at better, more robust, more nuanced representations of what we do know and a more honest picture of what we don’t. We hold the truth hostage from each other over the fear of puncturing our own thin skin.\nIn my imagination, we all might be better off if the status quo was exactly the opposite of the reaction I heard to Robert’s story. What if Robert didn’t fear losing any friends over his skepticism, and instead found that people actually gravitated towards that skepticism? What if doubt was sexy? What if humility was sexy? What if we lived in a world where we all agreed that Robert should lean further into his curiosity?"
  }
]