[
  {
    "objectID": "online-bayesian-updating.html",
    "href": "online-bayesian-updating.html",
    "title": "David Grayson",
    "section": "",
    "text": "Problem\nRecently I had to solve a problem that involved online learning, i.e. updating the weights of your model only with incoming batches of data. This is opposed to the much more familiar offline training paradigm where you cycle through multiple epochs over a single dataset.\nThere’s lots of potential use cases for this - think of any situation where you want your model continuously updating as data is simultaneously coming in (e.g. streaming apps, stock market prediction, etc.). The problem arises that the ‘right’ way of updating your model depends heavily on how much you want to balance new vs old data in terms of its influence on the the model. On top of this, even if you want to treat all data points equally, a new learning rate must still be chosen at each update, and this is non-trivial given that you don’t know anything about future samples.\n\n\nProposed solution\nThis post will cover arguably the simplest and most robust approach of using a bayesian framework to update the posterior distribution with each new batch of data. The key insight here is that bayesian updates are implemented quite naturally by simply resetting the prior distribution to whatever the posterior was, and then recalculating the new posterior based on the new data. The updated “weights” (the way we think of them in a frequentist sense) correspond to the mode of the most recently updated posterior. Note that this method does treat all data points equally, regardless of time, but that’s a simplifying assumption that’s helpful for now.\nHere I’ll give an example using a simple logistic regression.\nWith a model like this, the updates are easy (and fast!) to calculate via quadratic approximation of the posterior in PyMC. Later I’ll mention some more generalizable frameworks for approximating the posterior with neural networks.\n\n\nImport\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport pymc as pm\nimport seaborn as sns\nfrom scipy.stats import norm\n\nWARNING (aesara.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\n\n\nSimulate some data for a logistic regression\n(doesn’t matter how, as long as we can visualize a decision boundary)\nI’ll use 100 samples for simplicity\n\nn_obs = 100\n\nrng = np.random.default_rng()\nx0_obs = rng.normal(5, 2, size=n_obs)\ny_obs = rng.binomial(1, 0.5, size=n_obs)\n\ny_vec = np.zeros((len(y_obs), 2))\ny_vec[np.arange(len(y_obs)), y_obs] = 1\nmu_obs = y_vec @ np.array([1, 3]) + (y_vec @ np.array([1, 1])) * x0_obs\nx1_obs = rng.normal(mu_obs, 1)\n\nx0_obs = x0_obs - x0_obs.mean()\nx1_obs = x1_obs - x1_obs.mean()\nx_obs = np.concatenate(([np.ones(x0_obs.shape[0])], [x0_obs], [x1_obs])).T\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs)\nplt.show()\n\n\n\n\n\n\nSpecify the model & obtain the first posterior\nUsing just the first batch of 10 samples\n(and I’ll say to update the model a total of 10 times after that, 10 samples at a time)\n\nsteps = 10\n\nnum_features = x_obs.shape[1]\nstep = n_obs // steps\nx_start = x_obs[:step, :]\ny_start = y_obs[:step]\n\nwith pm.Model() as start_model:\n    # use Normal as priors\n    # w is our list of model weights\n    w = pm.Normal(\"w\", 0, 10, shape=num_features)\n    p = pm.math.invlogit(x_start @ w)\n    # likelihood function\n    y = pm.Binomial(\"y\", 1, p, observed=y_start)\n\n    # estimate the posterior as a gaussian\n    mean_w = pm.find_MAP()\n    hess = pm.find_hessian(mean_w, vars=[w])\n    var_cov = np.linalg.inv(hess)\n    std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:00<00:00 logp = -10.35, ||grad|| = 0.0050886]\n    \n    \n\n\n\n\n\n\n\nCode explanation\nLet me summarize what’s going on here:\nOur model has 3 weights (2 + bias), are denoted by w, and each given a prior with a mean of 0 and stdev of 10\nThe likelihood function is specified here as a bernoulli (or binomial with n=1) with p being a linear function of inputs * weights\nPyMC enables bayesian inference either through estimation or sampling of the posterior. In this case, we estimate it directly by finding the maximum of the posterior (analogous to maximum likelihood estimation) and describing its curvature with the hessian (a matrix of second-order partial derivatives). If we assume the shape of the posterior is gaussian, the hessian is sufficient to derive its standard deviation. Note that we could also sample the posterior instead, but that would be more expensive and won’t scale to n-dimensional problems.\nNotice then that the whole posterior for w is described by only two terms: mean_w and std_w (the mean/mode of a gaussian and its standard deviation). We can view their current values:\n\nprint(\"means:\", mean_w[\"w\"])\nprint(\"standard devs:\", std_w)\n\nmeans: [ 1.31026875 -5.99367736  6.3300888 ]\nstandard devs: [2.07364747 3.53659583 3.7744848 ]\n\n\n\n\nSubsequent updates\nFor subsequent updates, all we need is to reset our prior on w to the posterior we just found. The next posterior is discovered by simply repeating the same inference process on a new batch of data.\n\nmus, sigmas = [], []\n\nfor t in range(step, n_obs, step):\n    x_new = x_obs[t:t+step]\n    y_new = y_obs[t:t+step]\n    \n    with pm.Model() as updated_model:\n        # Reset priors to posteriors from previous iteration, unless weights are fixed\n        updated_mus = mean_w[\"w\"]\n        updated_sigmas = std_w\n        mus.append(updated_mus)\n        sigmas.append(updated_sigmas)\n        \n        w = pm.Normal(\"w\", updated_mus, updated_sigmas, shape=num_features)\n        p = pm.math.invlogit(x_new @ w)\n        y = pm.Binomial(\"y\", 1, p, observed=y_new)\n\n        mean_w = pm.find_MAP()\n        hess = pm.find_hessian(mean_w, vars=[w])\n        var_cov = np.linalg.inv(hess)\n        std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -6.6914, ||grad|| = 0.0018485]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -11.45, ||grad|| = 0.00042804]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -8.2299, ||grad|| = 2.5082]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [9/9 00:00<00:00 logp = -14.18, ||grad|| = 5.6728]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -3.598, ||grad|| = 1.3588]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.296, ||grad|| = 3.5075]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5569, ||grad|| = 2.7495]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5458, ||grad|| = 1.4423]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -6.5989, ||grad|| = 7.722]\n    \n    \n\n\n\n\n\nPlot changes to the posterior with each update\n\nmus = np.array(mus)\nsigmas = np.array(sigmas)\ncmap = matplotlib.cm.autumn\nx_axis = np.arange(-10, 10, 0.1)\n\nfor j in range(mus.shape[1]):\n    plt.figure(figsize=(4, 1))\n    plt.title(f\"w{j}\")\n    for c, (mu, sigma) in enumerate(zip(mus[:, j], sigmas[:, j])):\n        plt.plot(x_axis, norm.pdf(x_axis, mu, sigma), color=cmap(1 - c / mus.shape[0]))\n\n\n\n\n\n\n\n\n\n\nIn order to see the decision boundary, I need the weights as point estimates, so I just take the mean of the posterior.\nBelow shows how the decision boundary changed with each update.\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs.flatten())\n\nalpha0 = alpha = 1 / mus.shape[0]\nfor mu in mus:\n    w0, w1, w2 = mu\n    b = -w0/w2\n    m = -w1/w2\n    xd = np.array([x0_obs.min(), x0_obs.max()])\n    yd = m*xd + b\n    plt.plot(xd, yd, 'k', lw=1, ls='--', alpha=min(alpha, 1))\n    alpha += alpha0\n\nplt.show()\n\n\n\n\nThe figure shows the decision boundary change at each update, where the darker lines show the later updates. You can see the initial lines are a bit wonky, but as new data is added the updates converge on a good estimate of the boundary.\nAnd that’s it!\nFor further reading:\nJust as I’ve implemented this solution as a bayesian regression, the same approach could be taken using more complex (e.g. deep) models by implementing them as bayesian neural networks (e.g. check out pyro or this more lightweight and very impressive package). Because the parameters of a bayesian NN are described as posterior distributions, they could also be updated in an online fashion by continually resetting posteriors <-> priors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "How to make a personal blog using jupyter notebooks\n\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\n\n\n\n\n  \n\n\n\n\nOnline learning through bayesian updating\n\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\nEy yo\n\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m David. I’m a machine learning engineer and applied ML scientist. My interests include computer vision, generative AI, and deep learning in biotech. I am a big fan of mentorship and servant leadership. Some non-professional things I love include traveling to far-flung places with my bike and trying to set high scores on pinball machines :)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of California, Davis | Davis, CA | PhD in Neuroscience | Sept 2012 - May 2017\nCornell University | Ithaca, NY | B.A. in Computational Neuroscience | Aug 2004 - May 2008"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nBuzz Solutions | Senior ML Engineer | Jan 2023 - present\nLogic 20/20 | Lead Data Scientist | Jan 2021 - August 2022\nSystem1 Biosciences | Senior ML Scientist | Feb 2019 - April 2020\nIntuit | Senior Data Scientist - AI/ML | Sep 2017 - Jan 2019\n–"
  },
  {
    "objectID": "posts/online-bayesian-updating.html",
    "href": "posts/online-bayesian-updating.html",
    "title": "Bayesian updating with logistic regression",
    "section": "",
    "text": "Problem\nRecently I had to solve a problem that involved online learning, i.e. updating the weights of your model only with incoming batches of data. This is opposed to the much more familiar offline training paradigm where you cycle through multiple epochs over a single dataset.\nThere’s lots of potential use cases for this - think of any situation where you want your model continuously updating as data is simultaneously coming in (e.g. streaming apps, stock market prediction, etc.). The problem arises that the ‘right’ way of updating your model depends heavily on how much you want to balance new vs old data in terms of its influence on the the model. On top of this, even if you want to treat all data points equally, a new learning rate must still be chosen at each update, and this is non-trivial given that you don’t know anything about future samples.\n\n\nProposed solution\nThis post will cover arguably the simplest and most robust approach of using a bayesian framework to update the posterior distribution with each new batch of data. The key insight here is that bayesian updates are implemented quite naturally by simply resetting the prior distribution to whatever the posterior was, and then recalculating the new posterior based on the new data. The updated “weights” (the way we think of them in a frequentist sense) correspond to the mode of the most recently updated posterior. Note that this method does treat all data points equally, regardless of time, but that’s a simplifying assumption that’s helpful for now.\nHere I’ll give an example using a simple logistic regression.\nWith a model like this, the updates are easy (and fast!) to calculate via quadratic approximation of the posterior in PyMC. Later I’ll mention some more generalizable frameworks for approximating the posterior with neural networks.\n\n\nImport\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport pymc as pm\nimport seaborn as sns\nfrom scipy.stats import norm\n\n\n\nSimulate some data for a logistic regression\n(doesn’t matter how, as long as we can visualize a decision boundary)\nI’ll use 100 samples for simplicity\n\nn_obs = 100\n\nrng = np.random.default_rng()\nx0_obs = rng.normal(5, 2, size=n_obs)\ny_obs = rng.binomial(1, 0.5, size=n_obs)\n\ny_vec = np.zeros((len(y_obs), 2))\ny_vec[np.arange(len(y_obs)), y_obs] = 1\nmu_obs = y_vec @ np.array([1, 3]) + (y_vec @ np.array([1, 1])) * x0_obs\nx1_obs = rng.normal(mu_obs, 1)\n\nx0_obs = x0_obs - x0_obs.mean()\nx1_obs = x1_obs - x1_obs.mean()\nx_obs = np.concatenate(([np.ones(x0_obs.shape[0])], [x0_obs], [x1_obs])).T\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs)\nplt.show()\n\n\n\n\n\n\nSpecify the model & obtain the first posterior\nUsing just the first batch of 10 samples\n(and I’ll say to update the model a total of 10 times after that, 10 samples at a time)\n\nsteps = 10\n\nnum_features = x_obs.shape[1]\nstep = n_obs // steps\nx_start = x_obs[:step, :]\ny_start = y_obs[:step]\n\nwith pm.Model() as start_model:\n    # use Normal as priors\n    # w is our list of model weights\n    w = pm.Normal(\"w\", 0, 10, shape=num_features)\n    p = pm.math.invlogit(x_start @ w)\n    # likelihood function\n    y = pm.Binomial(\"y\", 1, p, observed=y_start)\n\n    # estimate the posterior as a gaussian\n    mean_w = pm.find_MAP()\n    hess = pm.find_hessian(mean_w, vars=[w])\n    var_cov = np.linalg.inv(hess)\n    std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:00<00:00 logp = -10.35, ||grad|| = 0.0050886]\n    \n    \n\n\n\n\n\n\n\nCode explanation\nLet me summarize what’s going on here:\nOur model has 3 weights (2 + bias), are denoted by w, and each given a prior with a mean of 0 and stdev of 10\nThe likelihood function is specified here as a bernoulli (or binomial with n=1) with p being a linear function of inputs * weights\nPyMC enables bayesian inference either through estimation or sampling of the posterior. In this case, we estimate it directly by finding the maximum of the posterior (analogous to maximum likelihood estimation) and describing its curvature with the hessian (a matrix of second-order partial derivatives). If we assume the shape of the posterior is gaussian, the hessian is sufficient to derive its standard deviation. Note that we could also sample the posterior instead, but that would be more expensive and won’t scale to n-dimensional problems.\nNotice then that the whole posterior for w is described by only two terms: mean_w and std_w (the mean/mode of a gaussian and its standard deviation). We can view their current values:\n\nprint(\"means:\", mean_w[\"w\"])\nprint(\"standard devs:\", std_w)\n\nmeans: [ 1.31026875 -5.99367736  6.3300888 ]\nstandard devs: [2.07364747 3.53659583 3.7744848 ]\n\n\n\n\nSubsequent updates\nFor subsequent updates, all we need is to reset our prior on w to the posterior we just found. The next posterior is discovered by simply repeating the same inference process on a new batch of data.\n\nmus, sigmas = [], []\n\nfor t in range(step, n_obs, step):\n    x_new = x_obs[t:t+step]\n    y_new = y_obs[t:t+step]\n    \n    with pm.Model() as updated_model:\n        # Reset priors to posteriors from previous iteration, unless weights are fixed\n        updated_mus = mean_w[\"w\"]\n        updated_sigmas = std_w\n        mus.append(updated_mus)\n        sigmas.append(updated_sigmas)\n        \n        w = pm.Normal(\"w\", updated_mus, updated_sigmas, shape=num_features)\n        p = pm.math.invlogit(x_new @ w)\n        y = pm.Binomial(\"y\", 1, p, observed=y_new)\n\n        mean_w = pm.find_MAP()\n        hess = pm.find_hessian(mean_w, vars=[w])\n        var_cov = np.linalg.inv(hess)\n        std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -6.6914, ||grad|| = 0.0018485]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -11.45, ||grad|| = 0.00042804]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -8.2299, ||grad|| = 2.5082]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [9/9 00:00<00:00 logp = -14.18, ||grad|| = 5.6728]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -3.598, ||grad|| = 1.3588]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.296, ||grad|| = 3.5075]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5569, ||grad|| = 2.7495]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5458, ||grad|| = 1.4423]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -6.5989, ||grad|| = 7.722]\n    \n    \n\n\n\n\n\nPlot changes to the posterior with each update\n\nmus = np.array(mus)\nsigmas = np.array(sigmas)\ncmap = matplotlib.cm.autumn\nx_axis = np.arange(-10, 10, 0.1)\n\nfor j in range(mus.shape[1]):\n    plt.figure(figsize=(4, 1))\n    plt.title(f\"w{j}\")\n    for c, (mu, sigma) in enumerate(zip(mus[:, j], sigmas[:, j])):\n        plt.plot(x_axis, norm.pdf(x_axis, mu, sigma), color=cmap(1 - c / mus.shape[0]))\n\n\n\n\n\n\n\n\n\n\nIn order to see the decision boundary, I need the weights as point estimates, so I just take the mean of the posterior.\nBelow shows how the decision boundary changed with each update.\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs.flatten())\n\nalpha0 = alpha = 1 / mus.shape[0]\nfor mu in mus:\n    w0, w1, w2 = mu\n    b = -w0/w2\n    m = -w1/w2\n    xd = np.array([x0_obs.min(), x0_obs.max()])\n    yd = m*xd + b\n    plt.plot(xd, yd, 'k', lw=1, ls='--', alpha=min(alpha, 1))\n    alpha += alpha0\n\nplt.show()\n\n\n\n\nThe figure shows the decision boundary change at each update, where the darker lines show the later updates. You can see the initial lines are a bit wonky, but as new data is added the updates converge on a good estimate of the boundary.\nAnd that’s it!\n\n\nFurther reading\nJust as I’ve implemented this solution as a bayesian regression, the same approach could be taken using more complex (e.g. deep) models by implementing them as bayesian neural networks (e.g. check out pyro or this more lightweight and very impressive package). Because the parameters of a bayesian NN are described as posterior distributions, they could also be updated in an online fashion by continually resetting posteriors <-> priors."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Ey yo",
    "section": "",
    "text": "I keep hearing from people about how important it is to blog. And I keep having ideas that I feel like I should put down on paper. So, here goes. Entry one. Done!"
  },
  {
    "objectID": "posts/online-bayesian-updating/online-bayesian-updating.html",
    "href": "posts/online-bayesian-updating/online-bayesian-updating.html",
    "title": "Online learning through bayesian updating",
    "section": "",
    "text": "Problem\nRecently I had to solve a machine learning problem that involved online learning, i.e. updating the weights of your model only with incoming batches of data. This is opposed to the much more familiar offline training paradigm where you cycle through multiple epochs over a single dataset.\nThere’s lots of potential use cases for this - think of any situation where you want your model continuously updating as data is simultaneously coming in (e.g. streaming apps, stock market prediction, etc.). The problem arises that the ‘right’ way of updating your model depends heavily on how much you want to balance new vs old data in terms of its influence on the the model. On top of this, even if you want to treat all data points equally, a new learning rate must still be chosen at each update, and this is non-trivial given that you don’t know anything about future samples.\nDo a quick search online and you will find lots of lengthy discussions of this problem as well as research papers that address it. Surprisingly, I found very few examples of simple proof-of-concepts with working code. Here I will aim to create just that.\n\n\nProposed solution\nThis post will show how to use bayesian inference to iteratively update a model’s weights with each new batch of data. The key insight here is that bayesian updates are implemented quite naturally by simply resetting the prior distribution to whatever the posterior was, and then recalculating the new posterior based on the new data. The updated “weights” (the way we think of them in a frequentist sense) correspond to the mode of the most recently updated posterior. Note that this method does treat all data points equally, regardless of time, but that’s a simplifying assumption that’s helpful for now.\nHere I’ll give an example using a simple logistic regression.\nWith a model like this, the updates are easy (and fast!) to calculate via quadratic approximation of the posterior in PyMC. Later I’ll mention some more generalizable frameworks for approximating the posterior with neural networks.\n\n\nImport\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport pymc as pm\nimport seaborn as sns\nfrom scipy.stats import norm\n\n\n\nSimulate some data for a logistic regression\n(doesn’t matter how, as long as we can visualize a decision boundary)\nI’ll use 100 samples for simplicity\n\nn_obs = 100\n\nrng = np.random.default_rng()\nx0_obs = rng.normal(5, 2, size=n_obs)\ny_obs = rng.binomial(1, 0.5, size=n_obs)\n\ny_vec = np.zeros((len(y_obs), 2))\ny_vec[np.arange(len(y_obs)), y_obs] = 1\nmu_obs = y_vec @ np.array([1, 3]) + (y_vec @ np.array([1, 1])) * x0_obs\nx1_obs = rng.normal(mu_obs, 1)\n\nx0_obs = x0_obs - x0_obs.mean()\nx1_obs = x1_obs - x1_obs.mean()\nx_obs = np.concatenate(([np.ones(x0_obs.shape[0])], [x0_obs], [x1_obs])).T\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs)\nplt.show()\n\n\n\n\n\n\nSpecify the model & obtain the first posterior\nUsing just the first batch of 10 samples\n(and I’ll say to update the model a total of 10 times after that, 10 samples at a time)\n\nsteps = 10\n\nnum_features = x_obs.shape[1]\nstep = n_obs // steps\nx_start = x_obs[:step, :]\ny_start = y_obs[:step]\n\nwith pm.Model() as start_model:\n    # use Normal as priors\n    # w is our list of model weights\n    w = pm.Normal(\"w\", 0, 10, shape=num_features)\n    p = pm.math.invlogit(x_start @ w)\n    # likelihood function\n    y = pm.Binomial(\"y\", 1, p, observed=y_start)\n\n    # estimate the posterior as a gaussian\n    mean_w = pm.find_MAP()\n    hess = pm.find_hessian(mean_w, vars=[w])\n    var_cov = np.linalg.inv(hess)\n    std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:00<00:00 logp = -10.35, ||grad|| = 0.0050886]\n    \n    \n\n\n\n\n\n\n\nCode explanation\nLet me summarize what’s going on here:\nOur model has 3 weights (2 + bias), are denoted by w, and each given a prior with a mean of 0 and stdev of 10\nThe likelihood function is specified here as a bernoulli (or binomial with n=1) with p being a sigmoid function of inputs * weights\nPyMC enables bayesian inference either through estimation or sampling of the posterior. In this case, we estimate it directly by finding the maximum of the posterior (analogous to maximum likelihood estimation) and describing its curvature with the hessian (a matrix of second-order partial derivatives). If we assume the shape of the posterior is gaussian, the hessian is sufficient to derive its standard deviation. Note that we could also sample the posterior instead, but that would be more expensive and won’t scale to n-dimensional problems.\nNotice then that the whole posterior for w is described by only two terms: mean_w and std_w (the mean/mode of a gaussian and its standard deviation). We can view their current values:\n\nprint(\"means:\", mean_w[\"w\"])\nprint(\"standard devs:\", std_w)\n\nmeans: [ 1.31026875 -5.99367736  6.3300888 ]\nstandard devs: [2.07364747 3.53659583 3.7744848 ]\n\n\n\n\nSubsequent updates\nFor subsequent updates, all we need is to reset our prior on w to the posterior we just found. The next posterior is discovered by simply repeating the same inference process on a new batch of data.\n\nmus, sigmas = [], []\n\nfor t in range(step, n_obs, step):\n    x_new = x_obs[t:t+step]\n    y_new = y_obs[t:t+step]\n    \n    with pm.Model() as updated_model:\n        # Reset priors to posteriors from previous iteration, unless weights are fixed\n        updated_mus = mean_w[\"w\"]\n        updated_sigmas = std_w\n        mus.append(updated_mus)\n        sigmas.append(updated_sigmas)\n        \n        w = pm.Normal(\"w\", updated_mus, updated_sigmas, shape=num_features)\n        p = pm.math.invlogit(x_new @ w)\n        y = pm.Binomial(\"y\", 1, p, observed=y_new)\n\n        mean_w = pm.find_MAP()\n        hess = pm.find_hessian(mean_w, vars=[w])\n        var_cov = np.linalg.inv(hess)\n        std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -6.6914, ||grad|| = 0.0018485]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -11.45, ||grad|| = 0.00042804]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -8.2299, ||grad|| = 2.5082]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [9/9 00:00<00:00 logp = -14.18, ||grad|| = 5.6728]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -3.598, ||grad|| = 1.3588]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.296, ||grad|| = 3.5075]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5569, ||grad|| = 2.7495]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5458, ||grad|| = 1.4423]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -6.5989, ||grad|| = 7.722]\n    \n    \n\n\n\n\n\nPlot changes to the posterior with each update\n\nmus = np.array(mus)\nsigmas = np.array(sigmas)\ncmap = matplotlib.cm.autumn\nx_axis = np.arange(-10, 10, 0.1)\n\nfor j in range(mus.shape[1]):\n    plt.figure(figsize=(4, 1))\n    plt.title(f\"w{j}\")\n    for c, (mu, sigma) in enumerate(zip(mus[:, j], sigmas[:, j])):\n        plt.plot(x_axis, norm.pdf(x_axis, mu, sigma), color=cmap(1 - c / mus.shape[0]))\n\n\n\n\n\n\n\n\n\n\nIn order to see the decision boundary, I need the weights as point estimates, so I just take the mean of the posterior.\nBelow shows how the decision boundary changed with each update.\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs.flatten())\n\nalpha0 = alpha = 1 / mus.shape[0]\nfor mu in mus:\n    w0, w1, w2 = mu\n    b = -w0/w2\n    m = -w1/w2\n    xd = np.array([x0_obs.min(), x0_obs.max()])\n    yd = m*xd + b\n    plt.plot(xd, yd, 'k', lw=1, ls='--', alpha=min(alpha, 1))\n    alpha += alpha0\n\nplt.show()\n\n\n\n\nThe figure shows the decision boundary change at each update, where the darker lines show the later updates. You can see the initial lines are a bit wonky, but as new data is added the updates converge on a good estimate of the boundary.\nAnd that’s it! Note that because it is actually the full posterior that is estimated, and not just the point estimate of its maximum, I can continue updating the model in perpetuity as new data arrives and still be able to recover the new decision boundary at any time.\n\n\nFurther reading\nJust as I’ve implemented this solution as a bayesian regression, the same approach could be taken using more complex (e.g. deep) models by implementing them as bayesian neural networks (e.g. check out pyro or this more lightweight and very impressive package). Because the parameters of a bayesian NN are described as posterior distributions, they could also be updated in an online fashion by continually resetting posteriors <-> priors."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "",
    "text": "So I made this personal website and blog after spending a fair amount of time researching how to publish a personal website and blog.\nSome options that were suggested to me:\n\nMedium\nWordpress\nGithub Pages + Jekyll\nGithub Pages + fastpages\n\nAfter realizing that I’d want tight integration between the tools I was using to produce content (i.e. jupyter notebooks) and the mechanism for publishing the content, I decided against Medium or Wordpress.\nGithub pages + jekyll was more what I wanted, but also turned up a few issues. 1. There was a fair amount to learn about jekyll to understand how to customize formatting. 2. It’s much easier to use an existing template, but the options out there generally are either not well-documented or not maintained, and 3. There’s a still an integration layer I’d need to manage in order to transfer code/outputs from notebooks to web content. I wanted SOME ability to customize, but mostly I wanted to get this thing up and running as fast as possible, and minimize potential work down the road.\nThose issues are what fastpages was designed to solve. It’s a well-documented, open-source package that enables automated rendering of content from a notebook into html content that is ready to publish, with some flexibility in how to render the content. It’s not a perfect solution - you’re still tied to their design templates. But there’s always a trade-off between customizability and ease of use, and this seemed like the trade-off that was right for me.\nBut, fastpages has been deprecated in favor of another open-source package."
  },
  {
    "objectID": "posts/01-27-2023-welcome/index.html",
    "href": "posts/01-27-2023-welcome/index.html",
    "title": "Ey yo",
    "section": "",
    "text": "I keep hearing from people about how important it is to blog. And I keep having ideas that I feel like I should put down on paper. So, here goes. Entry one. Done!"
  },
  {
    "objectID": "posts/01-28-2023-online-bayesian-updating/online-bayesian-updating.html",
    "href": "posts/01-28-2023-online-bayesian-updating/online-bayesian-updating.html",
    "title": "Online learning through bayesian updating",
    "section": "",
    "text": "Problem\nRecently I had to solve a machine learning problem that involved online learning, i.e. updating the weights of your model only with incoming batches of data. This is opposed to the much more familiar offline training paradigm where you cycle through multiple epochs over a single dataset.\nThere’s lots of potential use cases for this - think of any situation where you want your model continuously updating as data is simultaneously coming in (e.g. streaming apps, stock market prediction, etc.). The problem arises that the ‘right’ way of updating your model depends heavily on how much you want to balance new vs old data in terms of its influence on the the model. On top of this, even if you want to treat all data points equally, a new learning rate must still be chosen at each update, and this is non-trivial given that you don’t know anything about future samples.\nDo a quick search online and you will find lots of lengthy discussions of this problem as well as research papers that address it. Surprisingly, I found very few examples of simple proof-of-concepts with working code. Here I will aim to create just that.\n\n\nProposed solution\nThis post will show how to use bayesian inference to iteratively update a model’s weights with each new batch of data. The key insight here is that bayesian updates are implemented quite naturally by simply resetting the prior distribution to whatever the posterior was, and then recalculating the new posterior based on the new data. The updated “weights” (the way we think of them in a frequentist sense) correspond to the mode of the most recently updated posterior. Note that this method does treat all data points equally, regardless of time, but that’s a simplifying assumption that’s helpful for now.\nHere I’ll give an example using a simple logistic regression.\nWith a model like this, the updates are easy (and fast!) to calculate via quadratic approximation of the posterior in PyMC. Later I’ll mention some more generalizable frameworks for approximating the posterior with neural networks.\n\n\nImport\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport pymc as pm\nimport seaborn as sns\nfrom scipy.stats import norm\n\n\n\nSimulate some data for a logistic regression\n(doesn’t matter how, as long as we can visualize a decision boundary)\nI’ll use 100 samples for simplicity\n\nn_obs = 100\n\nrng = np.random.default_rng()\nx0_obs = rng.normal(5, 2, size=n_obs)\ny_obs = rng.binomial(1, 0.5, size=n_obs)\n\ny_vec = np.zeros((len(y_obs), 2))\ny_vec[np.arange(len(y_obs)), y_obs] = 1\nmu_obs = y_vec @ np.array([1, 3]) + (y_vec @ np.array([1, 1])) * x0_obs\nx1_obs = rng.normal(mu_obs, 1)\n\nx0_obs = x0_obs - x0_obs.mean()\nx1_obs = x1_obs - x1_obs.mean()\nx_obs = np.concatenate(([np.ones(x0_obs.shape[0])], [x0_obs], [x1_obs])).T\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs)\nplt.show()\n\n\n\n\n\n\nSpecify the model & obtain the first posterior\nUsing just the first batch of 10 samples\n(and I’ll say to update the model a total of 10 times after that, 10 samples at a time)\n\nsteps = 10\n\nnum_features = x_obs.shape[1]\nstep = n_obs // steps\nx_start = x_obs[:step, :]\ny_start = y_obs[:step]\n\nwith pm.Model() as start_model:\n    # use Normal as priors\n    # w is our list of model weights\n    w = pm.Normal(\"w\", 0, 10, shape=num_features)\n    p = pm.math.invlogit(x_start @ w)\n    # likelihood function\n    y = pm.Binomial(\"y\", 1, p, observed=y_start)\n\n    # estimate the posterior as a gaussian\n    mean_w = pm.find_MAP()\n    hess = pm.find_hessian(mean_w, vars=[w])\n    var_cov = np.linalg.inv(hess)\n    std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:00<00:00 logp = -10.35, ||grad|| = 0.0050886]\n    \n    \n\n\n\n\n\n\n\nCode explanation\nLet me summarize what’s going on here:\nOur model has 3 weights (2 + bias), are denoted by w, and each given a prior with a mean of 0 and stdev of 10\nThe likelihood function is specified here as a bernoulli (or binomial with n=1) with p being a sigmoid function of inputs * weights\nPyMC enables bayesian inference either through estimation or sampling of the posterior. In this case, we estimate it directly by finding the maximum of the posterior (analogous to maximum likelihood estimation) and describing its curvature with the hessian (a matrix of second-order partial derivatives). If we assume the shape of the posterior is gaussian, the hessian is sufficient to derive its standard deviation. Note that we could also sample the posterior instead, but that would be more expensive and won’t scale to n-dimensional problems.\nNotice then that the whole posterior for w is described by only two terms: mean_w and std_w (the mean/mode of a gaussian and its standard deviation). We can view their current values:\n\nprint(\"means:\", mean_w[\"w\"])\nprint(\"standard devs:\", std_w)\n\nmeans: [ 1.31026875 -5.99367736  6.3300888 ]\nstandard devs: [2.07364747 3.53659583 3.7744848 ]\n\n\n\n\nSubsequent updates\nFor subsequent updates, all we need is to reset our prior on w to the posterior we just found. The next posterior is discovered by simply repeating the same inference process on a new batch of data.\n\nmus, sigmas = [], []\n\nfor t in range(step, n_obs, step):\n    x_new = x_obs[t:t+step]\n    y_new = y_obs[t:t+step]\n    \n    with pm.Model() as updated_model:\n        # Reset priors to posteriors from previous iteration, unless weights are fixed\n        updated_mus = mean_w[\"w\"]\n        updated_sigmas = std_w\n        mus.append(updated_mus)\n        sigmas.append(updated_sigmas)\n        \n        w = pm.Normal(\"w\", updated_mus, updated_sigmas, shape=num_features)\n        p = pm.math.invlogit(x_new @ w)\n        y = pm.Binomial(\"y\", 1, p, observed=y_new)\n\n        mean_w = pm.find_MAP()\n        hess = pm.find_hessian(mean_w, vars=[w])\n        var_cov = np.linalg.inv(hess)\n        std_w = np.sqrt(np.diag(var_cov))\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -6.6914, ||grad|| = 0.0018485]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [13/13 00:00<00:00 logp = -11.45, ||grad|| = 0.00042804]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -8.2299, ||grad|| = 2.5082]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [9/9 00:00<00:00 logp = -14.18, ||grad|| = 5.6728]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -3.598, ||grad|| = 1.3588]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.296, ||grad|| = 3.5075]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5569, ||grad|| = 2.7495]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [7/7 00:00<00:00 logp = -4.5458, ||grad|| = 1.4423]\n    \n    \n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:00<00:00 logp = -6.5989, ||grad|| = 7.722]\n    \n    \n\n\n\n\n\nPlot changes to the posterior with each update\n\nmus = np.array(mus)\nsigmas = np.array(sigmas)\ncmap = matplotlib.cm.autumn\nx_axis = np.arange(-10, 10, 0.1)\n\nfor j in range(mus.shape[1]):\n    plt.figure(figsize=(4, 1))\n    plt.title(f\"w{j}\")\n    for c, (mu, sigma) in enumerate(zip(mus[:, j], sigmas[:, j])):\n        plt.plot(x_axis, norm.pdf(x_axis, mu, sigma), color=cmap(1 - c / mus.shape[0]))\n\n\n\n\n\n\n\n\n\n\nIn order to see the decision boundary, I need the weights as point estimates, so I just take the mean of the posterior.\nBelow shows how the decision boundary changed with each update.\n\nplt.figure(figsize=(4, 4))\nsns.scatterplot(x=x0_obs, y=x1_obs, hue=y_obs.flatten())\n\nalpha0 = alpha = 1 / mus.shape[0]\nfor mu in mus:\n    w0, w1, w2 = mu\n    b = -w0/w2\n    m = -w1/w2\n    xd = np.array([x0_obs.min(), x0_obs.max()])\n    yd = m*xd + b\n    plt.plot(xd, yd, 'k', lw=1, ls='--', alpha=min(alpha, 1))\n    alpha += alpha0\n\nplt.show()\n\n\n\n\nThe figure shows the decision boundary at each update, where the darker lines show the later updates. You can see the initial lines are a bit wonky, but as new data is added the updates converge on a good estimate of the boundary.\nAnd that’s it! Note that because it is actually the full posterior that is estimated, and not just the point estimate of its maximum, I can continue updating the model in perpetuity as new data arrives and still be able to recover point estimates of the weights at any time.\n\n\nFurther reading\nJust as I’ve implemented this solution as a bayesian regression, the same approach could be taken using more complex (e.g. deep) models by implementing them as bayesian neural networks (e.g. check out pyro or this more lightweight and very impressive package). Because the parameters of a bayesian NN are described as posterior distributions, they could also be updated in an online fashion by continually resetting posteriors <-> priors."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#solution",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#solution",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Solution",
    "text": "Solution\nEnter Quarto. This post will cover how to use Github Pages with Quarto in order to manage a personal website and blog with content that is generated entirely through jupyter notebooks. As you’ll see, what’s nice about this is that once you do the initial setup, all the content can be managed through nothing more than the notebooks themselves (if you’re into that sort of thing).\nSo let’s get to it."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-set-up-your-github-repo",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-set-up-your-github-repo",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 1: Set up your github repo",
    "text": "Step 1: Set up your github repo\nName it <username>.github.io and don’t include a README"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-create-your-github-repo",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-create-your-github-repo",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 1: Create your github repo",
    "text": "Step 1: Create your github repo\nName it <username>.github.io and don’t include a README. e.g.:\n[]create_gh_repo.png\nGo to the repo’s Pages settings and enable deployment of the main/master branch from the /docs folder\n[]edit_repo_settings.png"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-create-your-github-repo-1",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-1-create-your-github-repo-1",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 1: Create your github repo",
    "text": "Step 1: Create your github repo"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-2-initialize-your-quarto-site-locally",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-2-initialize-your-quarto-site-locally",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 2: Initialize your quarto site locally",
    "text": "Step 2: Initialize your quarto site locally\nclone your repo locally and create the initial quarto documents\n\ngit clone https://github.com/<username>/<username>.github.io\nquarto create-project <username>.github.io --type website:blog\n\nEdit the generated _quarto.yml file to use docs as the output-dir. It should read from the top:\nproject:\n  type: website\n  output-dir: docs\nThen, from the root of your repo, add a .nojekyll file to the root of your repository that tells GitHub Pages not to do additional processing of your published site using Jekyll\ntouch .nojekyll"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-3-add-some-content",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-3-add-some-content",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 3: add some content!",
    "text": "Step 3: add some content!\nFrom the root of your repo, begin rendering and previewing the pre-loaded content. As you add and edit your own content, your preview will update automatically.\n\n\nTerminal\n\nquarto preview"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-3-add-some-content-and-preview",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-3-add-some-content-and-preview",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 3: Add some content and preview",
    "text": "Step 3: Add some content and preview\nFrom the root of your repo, begin rendering and previewing the pre-loaded content in your browser. As you add and edit your own content, your preview will update automatically.\n\n\nTerminal\n\nquarto preview\n\nYou should see something that looks like this\n\nThe first thing to do is edit the title in the index.qmd file at the root level of your repo. You should see your update immediately.\n\n\nindex.qmd\n\n---\ntitle: \"<Your Name Here>\"\n\nNow you are ready to start adding posts. Unsurprisingly, all your posts should go in the posts folder.\nSo, in the posts folder, create a jupyter notebook, do stuff, and save. You should already see a new entry in your Home page on top of the other two pre-loaded posts.\nCool, right?\nN.B.: If you add a Raw cell to the top of your notebook, you can give it a title and date that will also appear on your site. Important: Make sure the cell is marked as “Raw” and not “Code” or “Markdown”. For example this is the header cell used in the notebook that this post comes from:\n\n\nposts/blog-post-about-posting-blogs.ipynb\n\n---\ntitle: \"How to make a personal blog using jupyter notebooks\"\ndate: \"2023-01-29\"\n---\n\nN.B.: To remove the pre-loaded posts, simply delete their folders in posts"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-push-to-github",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-push-to-github",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 4: Push to github",
    "text": "Step 4: Push to github\nYou can push all the content in your repo.\n\n\nTerminal\n\ngit add --all\ngit commit -m \"initial site\"\ngit push origin main"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-edit-your-homepage",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-edit-your-homepage",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 4: edit your homepage",
    "text": "Step 4: edit your homepage"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-5-push-to-github",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-5-push-to-github",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 5: Push to github",
    "text": "Step 5: Push to github\nFirst, add a .nojekyll file to the root of your repo that tells GitHub Pages not to do additional processing of your published site using Jekyll\n\n\nTerminal\n\ntouch .nojekyll\n\nThen, create a .gitignore file and add the output directory of your project to it:\n\n\n.gitignore\n\n/.quarto/\n/_site/\n\nNow you can push everything\n\n\nTerminal\n\ngit add --all\ngit commit -m \"initial site\"\ngit push origin main\n\n\nYour commit should automatically trigger a deployment through Github Actions. Once this completes, you should be able to see your site live at https://<username>.github.io\nAnd you’re done! You’ve now created a personal blog and can create additional posts by just creating new notebooks in the posts folder.\nTo learn more about how to customize your site appearance I highly recommend reading through quarto’s blog guide first. See also the full quarto guide."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-edit-your-about-page",
    "href": "posts/01-29-2023-how-to-make-a-blog/index.html#step-4-edit-your-about-page",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 4: Edit your “About” page",
    "text": "Step 4: Edit your “About” page\nFrom the root of your repo open about.qmd and add some content about yourself. You can read more here about how to customize this page to your liking"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "",
    "text": "So I made this personal website and blog after spending a fair amount of time researching how to publish a personal website and blog.\nSome options that were suggested to me:\n\nMedium\nWordpress\nGithub Pages + Jekyll\nGithub Pages + fastpages\n\nAfter realizing that I’d want tight integration between the tools I was using to produce content (i.e. jupyter notebooks) and the mechanism for publishing the content, I decided against Medium or Wordpress.\nGithub pages + jekyll was more what I wanted, but also turned up a few issues. 1. There was a fair amount to learn about jekyll to understand how to customize formatting. 2. It’s much easier to use an existing template, but the options out there generally are either not well-documented or not maintained, and 3. There’s a still an integration layer I’d need to manage in order to transfer code/outputs from notebooks to web content. I wanted SOME ability to customize, but mostly I wanted to get this thing up and running as fast as possible, and minimize potential work down the road.\nThose issues are what fastpages was designed to solve. It’s a well-documented, open-source package that enables automated rendering of content from a notebook into html content that is ready to publish, with some flexibility in how to render the content. It’s not a perfect solution - you’re still tied to their design templates. But there’s always a trade-off between customizability and ease of use, and this seemed like the trade-off that was right for me.\nBut, fastpages has been deprecated in favor of another open-source package."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#solution",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#solution",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Solution",
    "text": "Solution\nEnter Quarto. This post will cover how to use Github Pages with Quarto in order to manage a personal website and blog with content that is generated entirely through jupyter notebooks. It took me some time to get through all the docs in github and quarto that explain how to do this, so for your convenience I’m condensing the setup process I went through into one place here. As you’ll see, what’s nice is that after the initial setup, all the content can be managed through nothing more than the notebooks themselves (if you’re into that sort of thing).\nIn fact, this very blog was made using the same steps, and you can view its source code here.\nSo let’s get to it."
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-1-create-your-github-repo",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-1-create-your-github-repo",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 1: Create your github repo",
    "text": "Step 1: Create your github repo\nName it <username>.github.io. e.g.:\n\nThe URL for the site will be the same: https://username.github.io\nThen, go to the repo’s Pages settings and enable deployment of the main/master branch from the /docs folder"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-2-initialize-your-quarto-site-locally",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-2-initialize-your-quarto-site-locally",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 2: Initialize your quarto site locally",
    "text": "Step 2: Initialize your quarto site locally\nFirst, install quarto\nThen, clone your repo locally and create the initial quarto documents\n\n\nTerminal\n\ngit clone https://github.com/<username>/<username>.github.io\nquarto create-project <username>.github.io --type website:blog\ncd <username>.github.io\n\nReplacing <username> with your actual username of course\nFinally, edit the generated _quarto.yml file to use docs as the output-dir.\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-3-add-some-content-and-preview",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-3-add-some-content-and-preview",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 3: Add some content and preview",
    "text": "Step 3: Add some content and preview\nFirst, begin rendering and previewing the pre-loaded content in your browser. As you add and edit your own content, your preview will update automatically. From the root of your repo:\n\n\nTerminal\n\nquarto preview\n\nYou should see something that looks like this\n\n\n\nEdit the title in index.qmd. You should see your site update immediately.\n\n\nindex.qmd\n\n---\ntitle: \"<Your Name Here>\"\n\nNow you are ready to start adding posts. Unsurprisingly, all your posts should go in the posts folder.\nSo, in the posts folder, create a jupyter notebook, do stuff, and save. You should already see a new entry in your Home page on top of the other two pre-loaded posts. The notebook itself is used to render the post.\nCool, right?\nN.B.: If you add a Raw cell to the top of your notebook, you can give it a title and date that will also appear on your site. Important: Make sure the cell is marked as “Raw” and not “Code” or “Markdown”. For example this is the header cell used in the notebook that this post comes from:\n\n\nposts/blog-post-about-posting-blogs.ipynb\n\n---\ntitle: \"How to make a personal blog using jupyter notebooks\"\ndate: \"2023-01-29\"\n---\n\nN.B.: To remove the pre-loaded posts, simply delete their folders in posts"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-4-edit-your-about-page",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-4-edit-your-about-page",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 4: Edit your “About” page",
    "text": "Step 4: Edit your “About” page\nOpen about.qmd and add some content about yourself. You can read more here about how to customize this page to your liking"
  },
  {
    "objectID": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-5-push-to-github",
    "href": "posts/01-29-2023-how-to-make-a-blog/blog-post-about-posting-blogs.html#step-5-push-to-github",
    "title": "How to make a personal blog using jupyter notebooks",
    "section": "Step 5: Push to github",
    "text": "Step 5: Push to github\nFirst, add a .nojekyll file to the root of your repo that tells GitHub Pages not to do additional processing of your published site using Jekyll\n\n\nTerminal\n\ntouch .nojekyll\n\nThen, create a .gitignore file and add the output directory of your project to it:\n\n\n.gitignore\n\n/.quarto/\n/_site/\n\nNow you can push everything\n\n\nTerminal\n\ngit add --all\ngit commit -m \"initial site\"\ngit push origin main\n\n\nYour commit should automatically trigger a deployment through Github Actions. Once this completes, you should be able to see your site live at https://<username>.github.io\nAnd you’re done! You’ve now created a personal blog and can create additional posts by just creating new notebooks under posts.\nTo learn more about how to customize your site appearance I highly recommend reading through quarto’s blog guide first. See also the full quarto guide."
  }
]