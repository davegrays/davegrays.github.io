---
title: "Doctors are weak learners"
date: "2023-02-02"
categories:
---

Most of us have experienced a medical misdiagnosis of some kind. It is a frustrating experience that can result in enormous time wasted with unnecessary office visits and medical tests, plus significant stress and prolonged illness.

At the heart of the matter is the fact that many illnesses are still diagnosed using primarily human expertise with little understanding of the uncertainty behind those predictions. Even when lab tests are involved, interpretation of results and diagnostic conclusions still usually rely on the judgment of individual doctors. For instance, medical imaging such as MRIs or CT scans usually involves a trained radiologist reading the result and writing up a conclusion, which is then interpreted and communicated by the patient's provider. The diagnosis is therefore the result of expert intuition from a select pair of trained specialists. Equivalently, it is the result of these experts' combined knowledge and training, and also their lack of knowledge and lack of training.

It’s easy to see why this could be improved, and we have data that proves just how bad this process can be. Back in [2018](https://www.annalsofoncology.org/article/S0923-7534(19)34105-5/fulltext), it was first shown that AI outperforms most expert dermatologists, and reams of papers have come out since then improving on those results. If we go a bit further back to [2015](https://jamanetwork.com/journals/jamadermatology/fullarticle/2464932), we find solid evidence that individual doctors misdiagnose at much higher rates than they would if they combined their votes on each diagnosis (though this ‘collective intelligence’ is still much worse than AI). The authors argue that this effect is due to each doctor having their own biases - their own way of looking at the same signs, symptoms, and test results, and drawing their own unique and often incorrect conclusions.

This shouldn't really come as a surprise to anyone. Each doctor's understanding is innately limited by human abilities of perception and cognitive integration between diverse forms of information (e.g. patient records, symptoms, test results). In addition, each doctor can only practically be exposed to a (very) small subset of all the available patient data that might be useful for training in their area of expertise. In other words, each doctor can only fit their own limited mental model of disease to a limited sample of the data, leading to different doctors having different diagnostic models that are all suboptimal in their own unique way.

In the parlance of machine learning and artificial intelligence, these kinds of models are termed 'weak learners'. Concretely, a weak learner is an oversimplified statistical model that has limited mathematical complexity and often only allowed to 'see' a limited subset of data.

To make the analogy simple, one can think of each doctor as having built their own classification model that is trained to learn the often complex relationship between patient features (such as signs, symptoms, and test results) and patient diagnosis. This complex relationship must be learned through limited (i.e. weak) ability and experiential training.

What’s fascinating is that, in the ML literature, weak learners actually serve a very important role in producing very high performing models. The idea is that you can use an ensemble approach, which means training many weak learners on the same task and then making final predictions by taking a vote among your learners. In fact, for many types of problems, particularly those where the data is tabular, these types of models are consistently [the best performing](https://arxiv.org/abs/2106.03253). But the key takeaway is that you need to ensemble across many, many of these weak learners (i.e. hundreds or thousands depending on the size of the data). You also get another powerful boost to your model’s accuracy if you train the learners [sequentially](https://arxiv.org/pdf/1106.0257.pdf) so that each subsequent learner is an improvement of the previous.

One of the findings of the 2015 skin cancer study was that, indeed, if you ensemble the predictions of all doctors, you get predictions that are on average far better than those of any individual doctor's. Even so, AI now [vastly outperforms](https://doi.org/10.3390/app112210593) the best human experts at skin cancer detection as well as a host of other imaging based diagnostics. Thus, both theory and data would suggest that 'weak learner' is an apt description of the innate limitations of individual doctors' (read: any individual human’s) judgment and decision-making ability. The implication is that our error-prone healthcare system has been poorly serving patients by encouraging them to rely on their doctor’s opinion. That might be better than ceding authority to the ever expanding universe of medical quackery, but it’s much worse than providing data about how inaccurate doctors really are, and making an earnest effort to provide access to modern, science-based diagnostic approaches.
